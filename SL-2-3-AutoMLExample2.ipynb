{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fengfrankgthb/BUS-41204/blob/main/SL-2-3-AutoMLExample2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqoQZIRnDeeG"
      },
      "source": [
        "# Stacking and AutoML"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUhg54UIDhZ1"
      },
      "source": [
        "In this notebook, we'll illustrate stacking and automatic machine learning.\n",
        "\n",
        "The data are from S. Moro, P. Cortez and P. Rita (2014) “A Data-Driven Approach to Predict the Success of Bank Telemarketing. Decision Support Systems,” Decision Support Systems 62, 22-31.\n",
        "\n",
        "The outcome variable is a binary variable indicating whether a person subscribes to a bank term deposit (y = 1)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSvmVBy6D222"
      },
      "source": [
        "# Python setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MC4ApVC0GKsc"
      },
      "source": [
        "Note: If you want to re-run the AutoML code, you may need to downgrade NumPy and restart the session. After restarting, re-import NumPy. This step is only necessary if you intend to re-run the AutoML code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "foMs3xLPGKsc",
        "outputId": "51ddcd1b-dd28-47fe-fb70-8abf119abd58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 2.0.2\n",
            "Uninstalling numpy-2.0.2:\n",
            "  Successfully uninstalled numpy-2.0.2\n",
            "Collecting numpy==1.26.3\n",
            "  Downloading numpy-1.26.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.3\n"
          ]
        }
      ],
      "source": [
        "# Downgrade NumPy and then restart the session by clicking the Runtime tab\n",
        "!pip uninstall -y numpy\n",
        "!pip install numpy==1.26.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mo6VX92-D4kF"
      },
      "source": [
        "As usual, we'll start by importing libraries we're going to make use of."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POzgarpYjU-A"
      },
      "outputs": [],
      "source": [
        "!pip install flaml\n",
        "!pip install xgboost==1.6.0\n",
        "\n",
        "!pip install h2o\n",
        "\n",
        "# Import relevant packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.metrics import roc_curve, roc_auc_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "St6ZrtM5EC41"
      },
      "source": [
        "# Load and examine data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEMGtWL9EJMc"
      },
      "source": [
        "We'll import the data from the course github repository. We'll do the same data cleaning and pre-processing we did in [the previous notebook for this example.](https://colab.research.google.com/github/chansen776/MBA-ML-Course-Materials/blob/main/Code/BankDepositExample2.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVwdce0ujc0x"
      },
      "outputs": [],
      "source": [
        "# Import data\n",
        "file = \"https://raw.githubusercontent.com/chansen776/MBA-ML-Course-Materials/main/Data/bank-additional-full.csv\"\n",
        "rawdata = pd.read_csv(file, sep=\";\")\n",
        "print(rawdata.shape)\n",
        "print(rawdata.columns)\n",
        "print(rawdata.dtypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKjI1qJBjfAR"
      },
      "outputs": [],
      "source": [
        "# Recode outcome from \"yes\" and \"no\" to 1 and 0\n",
        "rawdata[\"y\"] = rawdata[\"y\"].replace({\"no\": 0, \"yes\": 1})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ev9WKiwfjhUJ"
      },
      "outputs": [],
      "source": [
        "# Create variable indicating not previously contacted and replace 999's in pdays with 0's\n",
        "rawdata[\"never_contacted\"] = np.where(rawdata[\"pdays\"] == 999, 1, 0)\n",
        "rawdata[\"never_contacted\"] = rawdata[\"never_contacted\"].astype(\"category\")\n",
        "rawdata[\"pdays\"] = np.where(rawdata[\"pdays\"] == 999, 0, rawdata[\"pdays\"])\n",
        "rawdata[\"pdays\"].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5nP7jVgjjqY"
      },
      "outputs": [],
      "source": [
        "# Drop duration column\n",
        "rawdata = rawdata.drop(columns=[\"duration\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6D2hPuWujlyg"
      },
      "outputs": [],
      "source": [
        "# Split the data into training (80%) and validation (20%) sets\n",
        "train, val = train_test_split(rawdata, test_size=0.2, random_state=94)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndnhtMtVE3at"
      },
      "source": [
        "# Stacking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjgwOA_OsaQp"
      },
      "source": [
        "It's hard to know the right model to try in any given setting.\n",
        "\n",
        "It is also possible that different models may perform differentially well for different instances in the same data. I.e. there may not be a \"best\" model.\n",
        "\n",
        "Rather than choose a single model, we might instead wish to combine the candidate models to try to obtain a better overall prediction rule.\n",
        "\n",
        "Stacking is one approach to building such a model combination. The basic idea is to take the (out-of-sample) predictions from each candidate model and simply use them as predictors in a new (usually simple) model aimed at predicting the outcome.\n",
        "\n",
        "For regression tasks, the stacking model is usually built by applying standard least squares linear regression to predict the outcome with the (out-of-sample) predictions from the baseline prediction rules as features.\n",
        "\n",
        "For binary classification tasks, a simple baseline is to build the stacking model by applying logistic regression to predict the outcome with the (out-of-sample) predictions from the baseline prediction rules as features.\n",
        "\n",
        "Here, we're going to apply stacking in the bank deposit example using the models [we previously tried.](https://colab.research.google.com/github/chansen776/MBA-ML-Course-Materials/blob/main/Code/BankDepositExample2.ipynb)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBm9xeLKjnvp"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "\n",
        "# Suppress only ConvergenceWarning\n",
        "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
        "\n",
        "# Prepare training and validation data\n",
        "X_train = train.drop(columns=[\"y\"])\n",
        "y_train = train[\"y\"]\n",
        "X_val = val.drop(columns=[\"y\"])\n",
        "y_val = val[\"y\"]\n",
        "\n",
        "# Get dummy variables for categorical features\n",
        "X_train_full = pd.get_dummies(X_train, drop_first=False)\n",
        "X_val_full = pd.get_dummies(X_val, drop_first=False)\n",
        "\n",
        "# Align validation data to ensure same columns as training data\n",
        "X_val_full = X_val_full.reindex(columns=X_train_full.columns, fill_value=0)\n",
        "\n",
        "# Prepare data specifically for Logistic Regression (drop one dummy variable per category)\n",
        "X_train_logistic = pd.get_dummies(X_train, drop_first=True)\n",
        "X_val_logistic = pd.get_dummies(X_val, drop_first=True)\n",
        "X_val_logistic = X_val_logistic.reindex(columns=X_train_logistic.columns, fill_value=0)\n",
        "\n",
        "\n",
        "# Define a custom wrapper to handle separate design matrices for stacking\n",
        "class CustomLogisticRegression:\n",
        "    def __init__(self, model, X_train, X_val):\n",
        "        self.model = model\n",
        "        self.X_train = X_train\n",
        "        self.X_val = X_val\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.model.fit(self.X_train, y)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.model.predict(self.X_val)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        return self.model.predict_proba(self.X_val)\n",
        "\n",
        "\n",
        "# Instantiate individual models\n",
        "logistic_model = CustomLogisticRegression(\n",
        "    model=LogisticRegression(max_iter=1000, random_state=94, penalty=None),\n",
        "    X_train=X_train_logistic,\n",
        "    X_val=X_val_logistic,\n",
        ")\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=1000, min_samples_leaf=15, random_state=94\n",
        ")\n",
        "gbc_model = GradientBoostingClassifier(\n",
        "    random_state=94, learning_rate=0.1, max_depth=4, n_estimators=60\n",
        ")\n",
        "\n",
        "gbcES = GradientBoostingClassifier(\n",
        "    learning_rate=0.1,\n",
        "    max_depth=4,\n",
        "    n_estimators=200,\n",
        "    validation_fraction=0.2,\n",
        "    n_iter_no_change=10,\n",
        "    tol=1e-4,\n",
        "    random_state=94,\n",
        ")\n",
        "\n",
        "\n",
        "# Constant model\n",
        "def constant_model_predict_proba(X):\n",
        "    return np.full((X.shape[0], 2), [1 - y_train.mean(), y_train.mean()])\n",
        "\n",
        "\n",
        "def constant_model_predict(X):\n",
        "    return np.zeros(X.shape[0])\n",
        "\n",
        "\n",
        "class ConstantModel(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self):\n",
        "        pass  # No need for self.mean as it's calculated in fit\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.mean = y.mean()\n",
        "        self.classes_ = np.unique(y)\n",
        "        return self  # Return self for compatibility with scikit-learn estimators\n",
        "\n",
        "    def get_params(self, deep=True):\n",
        "        # This method is required for cloning and compatibility with scikit-learn\n",
        "        return {}  # ConstantModel has no parameters to return\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        return constant_model_predict_proba(X)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return constant_model_predict(X)\n",
        "\n",
        "\n",
        "constant_model = ConstantModel()\n",
        "constant_model.fit(X_train_full, y_train)\n",
        "\n",
        "# Create a stacking classifier with Logistic Regression as the meta-model\n",
        "stacking_model = StackingClassifier(\n",
        "    estimators=[\n",
        "        (\n",
        "            \"logistic\",\n",
        "            logistic_model.model,\n",
        "        ),  # Logistic regression with reduced design matrix\n",
        "        (\"random_forest\", rf_model),\n",
        "        (\"gradient_boosting\", gbc_model),\n",
        "        (\"gradient_boosting_es\", gbcES),\n",
        "        (\"constant\", constant_model),\n",
        "    ],\n",
        "    final_estimator=LogisticRegression(max_iter=1000, random_state=94),\n",
        ")\n",
        "\n",
        "# Fit the stacking model with full design matrix for meta-model\n",
        "stacking_model.fit(X_train_full, y_train)\n",
        "\n",
        "# Make predictions on the validation set\n",
        "y_pred_stack = stacking_model.predict(X_val_full)\n",
        "y_pred_prob_stack = stacking_model.predict_proba(X_val_full)[:, 1]\n",
        "\n",
        "# Evaluate the stacking model\n",
        "stacking_classification_metrics = classification_report(\n",
        "    y_val, y_pred_stack, output_dict=True\n",
        ")\n",
        "print(pd.DataFrame(stacking_classification_metrics))\n",
        "\n",
        "ConfusionMatrixDisplay.from_predictions(y_val, y_pred_stack)\n",
        "plt.show()\n",
        "\n",
        "# ROC Curve\n",
        "fpr_stack, tpr_stack, thresholds_stack = roc_curve(y_val, y_pred_prob_stack)\n",
        "roc_auc_stack = roc_auc_score(y_val, y_pred_prob_stack)\n",
        "\n",
        "plt.plot(fpr_stack, tpr_stack, label=f\"Stacking (area = {roc_auc_stack:.2f})\")\n",
        "\n",
        "# Plot individual models for comparison\n",
        "# Logistic Regression\n",
        "logistic_model.model.fit(X_train_logistic, y_train)\n",
        "y_pred_prob_logistic = logistic_model.model.predict_proba(X_val_logistic)[:, 1]\n",
        "fpr_logistic, tpr_logistic, _ = roc_curve(y_val, y_pred_prob_logistic)\n",
        "roc_auc_logistic = roc_auc_score(y_val, y_pred_prob_logistic)\n",
        "plt.plot(\n",
        "    fpr_logistic,\n",
        "    tpr_logistic,\n",
        "    label=f\"Logistic Regression (area = {roc_auc_logistic:.2f})\",\n",
        ")\n",
        "\n",
        "# Random Forest\n",
        "rf_model.fit(X_train_full, y_train)\n",
        "y_pred_prob_rf = rf_model.predict_proba(X_val_full)[:, 1]\n",
        "fpr_rf, tpr_rf, _ = roc_curve(y_val, y_pred_prob_rf)\n",
        "roc_auc_rf = roc_auc_score(y_val, y_pred_prob_rf)\n",
        "plt.plot(fpr_rf, tpr_rf, label=f\"Random Forest (area = {roc_auc_rf:.2f})\")\n",
        "\n",
        "# Gradient Boosting\n",
        "gbc_model.fit(X_train_full, y_train)\n",
        "y_pred_prob_gbc = gbc_model.predict_proba(X_val_full)[:, 1]\n",
        "fpr_gbc, tpr_gbc, _ = roc_curve(y_val, y_pred_prob_gbc)\n",
        "roc_auc_gbc = roc_auc_score(y_val, y_pred_prob_gbc)\n",
        "plt.plot(fpr_gbc, tpr_gbc, label=f\"Gradient Boosting (area = {roc_auc_gbc:.2f})\")\n",
        "\n",
        "# Gradient Boosting with Early Stopping\n",
        "gbcES.fit(X_train_full, y_train)\n",
        "y_pred_prob_gbcES = gbcES.predict_proba(X_val_full)[:, 1]\n",
        "fpr_gbcES, tpr_gbcES, _ = roc_curve(y_val, y_pred_prob_gbcES)\n",
        "roc_auc_gbcES = roc_auc_score(y_val, y_pred_prob_gbcES)\n",
        "plt.plot(\n",
        "    fpr_gbcES,\n",
        "    tpr_gbcES,\n",
        "    label=f\"Gradient Boosting Early Stopping (area = {roc_auc_gbcES:.2f})\",\n",
        ")\n",
        "\n",
        "# Constant Model\n",
        "y_pred_prob_constant = constant_model.predict_proba(X_val_full)[:, 1]\n",
        "fpr_constant, tpr_constant, _ = roc_curve(y_val, y_pred_prob_constant)\n",
        "roc_auc_constant = roc_auc_score(y_val, y_pred_prob_constant)\n",
        "plt.plot(\n",
        "    fpr_constant, tpr_constant, label=f\"Constant Model (area = {roc_auc_constant:.2f})\"\n",
        ")\n",
        "\n",
        "# Finalize the plot\n",
        "plt.plot([0, 1], [0, 1], color=\"black\", lw=2, linestyle=\"--\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gI6CzVzPkAII"
      },
      "outputs": [],
      "source": [
        "# Weights ordered same way as learners - logistic, random forest,\n",
        "# boosting, boosting early stopping, constant\n",
        "stacking_weights = (\n",
        "    stacking_model.final_estimator_.coef_\n",
        "    if hasattr(stacking_model.final_estimator_, \"coef_\")\n",
        "    else None\n",
        ")\n",
        "if stacking_weights is not None:\n",
        "    print(\"Stacking Weights:\")\n",
        "    print(stacking_weights)\n",
        "else:\n",
        "    print(\"Final estimator does not support coefficients.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOiUxaIeE61X"
      },
      "source": [
        "# FLAML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7_AWizakIJg"
      },
      "outputs": [],
      "source": [
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "from flaml import AutoML\n",
        "\n",
        "X_train = train.drop(columns=[\"y\"])\n",
        "y_train = train[\"y\"]\n",
        "X_val = val.drop(columns=[\"y\"])\n",
        "y_val = val[\"y\"]\n",
        "\n",
        "# Get dummy variables for our categorical features.\n",
        "X_train = pd.get_dummies(X_train, drop_first=False)\n",
        "X_val = pd.get_dummies(X_val, drop_first=False)\n",
        "\n",
        "model_config = {\n",
        "    \"task\": \"classification\",\n",
        "    \"time_budget\": 300,  # time budget in seconds\n",
        "    \"eval_method\": \"cv\",\n",
        "    \"split_type\": KFold(n_splits=5, shuffle=True, random_state=94),\n",
        "    \"ensemble\": True,\n",
        "    \"verbose\": 0,  # only display warnings\n",
        "}\n",
        "\n",
        "\n",
        "automl = AutoML()\n",
        "automl.fit(X_train, y_train, **model_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dSrtjxhJkO5g"
      },
      "outputs": [],
      "source": [
        "print(automl.model)\n",
        "print(\n",
        "    \"The \",\n",
        "    automl.best_iteration,\n",
        "    \"-th iteration is the best, completed in \",\n",
        "    round(automl.time_to_find_best_model, 1),\n",
        "    \" seconds.\",\n",
        "    sep=\"\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjoqyowhkPRI"
      },
      "outputs": [],
      "source": [
        "y_pred_flaml = automl.predict(X_val)\n",
        "y_pred_prob_flaml = automl.predict_proba(X_val)[:, 1]\n",
        "\n",
        "# Evaluate the model\n",
        "aml_classification_metrics = classification_report(\n",
        "    y_val, y_pred_flaml, output_dict=True\n",
        ")\n",
        "print(pd.DataFrame(aml_classification_metrics))\n",
        "\n",
        "ConfusionMatrixDisplay.from_predictions(y_val, y_pred_flaml)\n",
        "plt.show()\n",
        "\n",
        "# ROC Curve\n",
        "fpr_flaml, tpr_flaml, thresholds_flaml = roc_curve(y_val, y_pred_prob_flaml)\n",
        "roc_auc_flaml = roc_auc_score(y_val, y_pred_prob_flaml)\n",
        "\n",
        "plt.plot(\n",
        "    fpr_logistic,\n",
        "    tpr_logistic,\n",
        "    label=f\"Logistic Regression (area = {roc_auc_logistic:.2f})\",\n",
        ")\n",
        "plt.plot(fpr_rf, tpr_rf, label=f\"Random Forest (area = {roc_auc_rf:.2f})\")\n",
        "plt.plot(fpr_gbc, tpr_gbc, label=f\"Gradient Boosting (area = {roc_auc_gbc:.2f})\")\n",
        "plt.plot(\n",
        "    fpr_gbcES, tpr_gbcES, label=f\"Gradient Boosting Early (area = {roc_auc_gbcES:.2f})\"\n",
        ")\n",
        "plt.plot(fpr_flaml, tpr_flaml, label=f\"FLAML (area = {roc_auc_flaml:.2f})\")\n",
        "plt.plot([0, 1], [0, 1], color=\"black\", lw=2, linestyle=\"--\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGxZxdZXE-Xk"
      },
      "source": [
        "#H2o"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rt8vOyHIkXKn"
      },
      "outputs": [],
      "source": [
        "import h2o\n",
        "from h2o.automl import H2OAutoML\n",
        "\n",
        "h2o.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQGm-QWIkbMJ"
      },
      "outputs": [],
      "source": [
        "# Convert pandas DataFrame to H2OFrame\n",
        "train_h2o = h2o.H2OFrame(train)\n",
        "val_h2o = h2o.H2OFrame(val)\n",
        "\n",
        "# Need to convert y variable to factor if going to use h2o\n",
        "train_h2o[\"y\"] = train_h2o[\"y\"].asfactor()\n",
        "val_h2o[\"y\"] = val_h2o[\"y\"].asfactor()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Njji4c8kkfHf"
      },
      "outputs": [],
      "source": [
        "# Takes ~ 30 minutes to run\n",
        "\n",
        "X_train = train.drop(columns=[\"y\"])\n",
        "y_train = train[\"y\"]\n",
        "X_val = val.drop(columns=[\"y\"])\n",
        "y_val = val[\"y\"]\n",
        "\n",
        "# Run AutoML for 20 base models\n",
        "aml = H2OAutoML(max_models=20, seed=42, nfolds=5)\n",
        "aml.train(x=list(X_train.columns), y=y_train.name, training_frame=train_h2o)\n",
        "\n",
        "# View the AutoML Leaderboard\n",
        "lb = aml.leaderboard\n",
        "lb.head(rows=lb.nrows)  # Print all rows instead of default (10 rows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5H6Kb_S2kf2Y"
      },
      "outputs": [],
      "source": [
        "# Evaluate the best model on the test data\n",
        "y_pred_aml_all = aml.leader.predict(val_h2o)  # Returns 3 columns.\n",
        "# First column = binary prediction\n",
        "# Second column = probability y = 0\n",
        "# Third column = probability y = 1\n",
        "\n",
        "y_pred_aml = y_pred_aml_all[:, 0].as_data_frame(use_pandas=True).to_numpy()\n",
        "y_pred_prob_aml = y_pred_aml_all[:, 2].as_data_frame(use_pandas=True).to_numpy()\n",
        "\n",
        "# Evaluate the model\n",
        "aml_classification_metrics = classification_report(y_val, y_pred_aml, output_dict=True)\n",
        "print(pd.DataFrame(aml_classification_metrics))\n",
        "\n",
        "ConfusionMatrixDisplay.from_predictions(y_val, y_pred_aml)\n",
        "plt.show()\n",
        "\n",
        "# ROC Curve\n",
        "fpr_aml, tpr_aml, thresholds_aml = roc_curve(y_val, y_pred_prob_aml)\n",
        "roc_auc_aml = roc_auc_score(y_val, y_pred_prob_aml)\n",
        "\n",
        "plt.plot(\n",
        "    fpr_logistic,\n",
        "    tpr_logistic,\n",
        "    label=f\"Logistic Regression (area = {roc_auc_logistic:.2f})\",\n",
        ")\n",
        "plt.plot(fpr_rf, tpr_rf, label=f\"Random Forest (area = {roc_auc_rf:.2f})\")\n",
        "plt.plot(fpr_gbc, tpr_gbc, label=f\"Gradient Boosting (area = {roc_auc_gbc:.2f})\")\n",
        "plt.plot(\n",
        "    fpr_gbcES, tpr_gbcES, label=f\"Gradient Boosting Early (area = {roc_auc_gbcES:.2f})\"\n",
        ")\n",
        "plt.plot(fpr_flaml, tpr_flaml, label=f\"FLAML (area = {roc_auc_flaml:.2f})\")\n",
        "plt.plot(fpr_aml, tpr_aml, label=f\"AML H2o (area = {roc_auc_aml:.2f})\")\n",
        "plt.plot([0, 1], [0, 1], color=\"black\", lw=2, linestyle=\"--\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}