{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fengfrankgthb/BUS-41204/blob/main/SL-2-1-FlowerChurnExample2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification basics using florist churn data"
      ],
      "metadata": {
        "id": "BJzC_yvQxieY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we'll illustrate classification using logistic regression with the main goal being to define common classification evaluation measures.\n",
        "\n",
        "The data were downloaded from https://huggingface.co/ and may be artificial. However, they serve to illustrate the points we want to learn about.\n",
        "\n",
        "If the data are real, they have likely been *balanced* by undersampling the majority class (customers that did not *churn*). The other alternative is that the florist is really bad at retaining customers. Either way, we're going to ignore this as it is tangential to the point of the exercise.\n",
        "\n"
      ],
      "metadata": {
        "id": "Vzxo_QMlxnWF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Python libraries"
      ],
      "metadata": {
        "id": "pHoecnd81IMV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As usual, we'll start by importing libraries we're going to make use of."
      ],
      "metadata": {
        "id": "Dd5iEu6w1K_k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rprQ-yqCxgu6"
      },
      "outputs": [],
      "source": [
        "# Import relevant packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.tree import plot_tree\n",
        "from sklearn.model_selection import KFold, GridSearchCV"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import and examine data"
      ],
      "metadata": {
        "id": "7xGDL7YC1nbA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll import the data from the course github repository."
      ],
      "metadata": {
        "id": "VIjz5U721yUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file = \"https://raw.githubusercontent.com/chansen776/MBA-ML-Course-Materials/main/Data/florist_customer_churn.csv\"\n",
        "data = pd.read_csv(file)"
      ],
      "metadata": {
        "id": "rI9LHdDK1vV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see what we've got in the dataset."
      ],
      "metadata": {
        "id": "YkmptmRv2BS7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.columns)\n",
        "data.head()"
      ],
      "metadata": {
        "id": "PyA4TusE2EE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The outcome variable is `churn`. We see that we have mostly qualitative data including the column `feedback` which is a basic example of text data that we'll need to deal with.\n",
        "\n",
        "Note that `total_charges` = `tenure`*`monthly_charges`."
      ],
      "metadata": {
        "id": "iwG5lS6L2Wjg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's calculate the correlation of total_charges with tenure*monthly_charges\n",
        "print('Correlation of total_charges with tenure*monthly_charges:',\n",
        "      data['total_charges'].corr(data['tenure']*data['monthly_charges']))"
      ],
      "metadata": {
        "id": "Uk8gHwJa24GG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see what our variables (outside of `feedback`) look like."
      ],
      "metadata": {
        "id": "pzmtfKkb4Bmu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# histogram of tenure\n",
        "sns.displot(data=data, x='tenure', bins = 10)\n",
        "plt.title('Histogram of Tenure')\n",
        "plt.show()\n",
        "\n",
        "# histogram of monthly_charges\n",
        "sns.displot(data=data, x='monthly_charges', bins = 10)\n",
        "plt.title('Histogram of Monthly Charges')\n",
        "plt.show()\n",
        "\n",
        "# histogram of total_charges\n",
        "sns.displot(data=data, x='total_charges', bins = 10)\n",
        "plt.title('Histogram of Total Charges')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "RK9QQ5Jw4Oso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's tabulate the qualitative variables."
      ],
      "metadata": {
        "id": "0gbYW6Fw6DC9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['churn'].value_counts()"
      ],
      "metadata": {
        "id": "kuGmCc2u6IDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As noted above, we certainly hope this is not a representative snapshot of a real firm's customer retention."
      ],
      "metadata": {
        "id": "AkdkiKdb6REJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(data['contract'].value_counts())\n",
        "print(sum((data['contract']).isna()))"
      ],
      "metadata": {
        "id": "Wr-UJD9r6ijy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have three types of contracts. We'll include these as dummy variables."
      ],
      "metadata": {
        "id": "8PF1hn4h6pP5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(data['payment_method'].value_counts())\n",
        "print(sum((data['payment_method']).isna()))"
      ],
      "metadata": {
        "id": "O6wvAg3G6tLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Four payment methods, which we'll include as dummies."
      ],
      "metadata": {
        "id": "EpvWaszh62sE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(data['topic'].value_counts())\n",
        "print(sum((data['topic']).isna()))"
      ],
      "metadata": {
        "id": "Ku3fPLcA67u2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have 26 observations that do not have a topic recorded. We'll treat those observations as just belonging to a different \"null\" topic."
      ],
      "metadata": {
        "id": "ybXty_Sf67D1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace non-string values in the 'topic' column with an empty string\n",
        "data['topic'] = data['topic'].fillna('')\n",
        "print(data['topic'].value_counts())\n",
        "print(sum((data['topic']).isna()))"
      ],
      "metadata": {
        "id": "teTkotSn8Fb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also have some empty values in `feedback`. We're not going to tabulate feedback, but we will assign these observations to have a \"null\" feedback."
      ],
      "metadata": {
        "id": "AoZxb_V48OX1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace non-string values in the 'feedback' column with an empty string\n",
        "data['feedback'] = data['feedback'].fillna('')\n",
        "print(sum(data['feedback'].isna()))"
      ],
      "metadata": {
        "id": "NSaXfCgK8bP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classifying sentiment\n",
        "\n",
        "We want to use the customer feedback information, but we certainly can't use the raw text and probably don't want to make dummy variables for each unique value (as there are many different phrases). Instead we are going to use a pre-trained large language model to construct the *sentiment* of each string.\n",
        "\n",
        "Specifically, we are going to use a [sentiment classification model that was fine tuned using Amazon reviews available on hugging face](https://huggingface.co/AdamCodd/distilbert-base-uncased-finetuned-sentiment-amazon).\n",
        "\n",
        "This model will return a prediction of \"positive\" or \"negative\" for each customer feedback phrase. It will also return a score between 0 and 1 (the predicted probability that the feedback was positive) that we can think about using as a continuout feature in our model to predict churn.\n",
        "\n",
        "Using the text data in this way is an example of *feature engineering*. We'll talk more about feature engineering as we go through the course."
      ],
      "metadata": {
        "id": "V_S7LJyt8wKV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "sentiment_analysis = pipeline(\"sentiment-analysis\",\n",
        "                              model=\"AdamCodd/distilbert-base-uncased-finetuned-sentiment-amazon\")"
      ],
      "metadata": {
        "id": "3MimiRwQwn7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at how the imported sentiment model assigns sentiment to some phrases."
      ],
      "metadata": {
        "id": "-nFk2cbfqWqY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# \"random\" phrases\n",
        "print(sentiment_analysis(\"I love this!\"))\n",
        "print(sentiment_analysis(\"I hate this!\"))\n",
        "print(sentiment_analysis(\"Things are ok.\"))\n",
        "print('\\n')\n",
        "\n",
        "# A phrase from our data\n",
        "print(data['feedback'][10])\n",
        "sentiment_analysis(data['feedback'][10])"
      ],
      "metadata": {
        "id": "cB6TWS68qV3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're now going to generate sentiment scores for all the phrases in our data."
      ],
      "metadata": {
        "id": "7xJPU0_hkwcE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct sentiment \"probabilities\" and label\n",
        "sentprobs = []\n",
        "sent = []\n",
        "for i in range(len(data)):\n",
        "  tmp = sentiment_analysis(data['feedback'][i])\n",
        "  sent.append(tmp[0]['label'])\n",
        "  if tmp[0]['label'] == 'negative':\n",
        "    sentprobs.append(1-tmp[0]['score'])\n",
        "  else:\n",
        "    sentprobs.append(tmp[0]['score'])\n",
        "\n",
        "# Make into data and merge with the original data\n",
        "sentprobs = pd.DataFrame(sentprobs, columns=['polarity'])\n",
        "sent = pd.DataFrame(sent, columns=['sentiment'])\n",
        "data = pd.concat([data, sentprobs, sent], axis=1)\n"
      ],
      "metadata": {
        "id": "WJoEC298x2YR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at the new variables."
      ],
      "metadata": {
        "id": "NRA-uWKTzn6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Positive/Negative sentiment\n",
        "print(data['sentiment'].value_counts())\n",
        "\n",
        "# Polarity\n",
        "print(data['polarity'].describe())\n",
        "\n",
        "# histogram of polarity\n",
        "sns.displot(data=data, x='polarity')\n",
        "plt.title('Histogram of Polarity')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "D-IWRG52zr6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall that we had some people return no text response. They should not be labeled \"positive\" or \"negative\". Let's relabel them as \"No Feedback.\""
      ],
      "metadata": {
        "id": "bWhGfHyh0Mxg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(data[data['feedback'] == '']['sentiment'].value_counts())\n",
        "print('\\n')\n",
        "\n",
        "data.loc[data['feedback'] == '', 'sentiment'] = 'No Feedback'\n",
        "print(data['sentiment'].value_counts())"
      ],
      "metadata": {
        "id": "LqlYc8jlI1I3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'd really like to verify that the sentiment extracted makes sense by doing human verification (or even better training our own sentiment model based on human feedback). We're going to just do some sanity checking by looking at polarity assigned to some of our phrases."
      ],
      "metadata": {
        "id": "fmcDSIDAAoNF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Phrases ranked in the bottom 10 of positivity\n",
        "data[data['polarity'].rank(method = 'min') < 10][['feedback','polarity']]"
      ],
      "metadata": {
        "id": "V6czAaT2LBKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Phrases associated ranked in the top 10 of positivity\n",
        "data[data['polarity'].rank(method = 'max') > 990][['feedback','polarity']]"
      ],
      "metadata": {
        "id": "pQhILYCdBAMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Word cloud of the positive sentiment feedback\n",
        "text = ' '.join(data[data['sentiment'] == 'positive']['feedback'].dropna().astype(str).tolist())\n",
        "\n",
        "# Generate the word cloud\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
        "\n",
        "# Display the word cloud using matplotlib\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')  # Turn off the axis\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IAh_d69s2vWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Word cloud of the negative sentiment feedback\n",
        "text = ' '.join(data[data['sentiment'] == 'negative']['feedback'].dropna().astype(str).tolist())\n",
        "\n",
        "# Generate the word cloud\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
        "\n",
        "# Display the word cloud using matplotlib\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')  # Turn off the axis\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6NHY6u4n3dBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train/Test Split\n",
        "\n",
        "We are going to use a simple train/test split to validate our models (rather than bother with cross-validation). We also don't have much data, so we won't keep aside a separate test data set."
      ],
      "metadata": {
        "id": "Sz5T2IOW5Kkt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training (80%) and validation (20%) sets\n",
        "train_data, validation_data = train_test_split(data, test_size=0.2, random_state=726)"
      ],
      "metadata": {
        "id": "CHunt2ON4_Eg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression\n",
        "\n",
        "We are going to look at predicting churn using a baseline classification model: *Logistic Regression*.\n",
        "\n",
        "Logistic regression builds a model for the probability that the outcome variable equals 1. In our example, this will correspond to the probability that a customer leaves (`churn` = `True`).\n",
        "\n",
        "Logistic regression specifically builds a linear combination of the feature variables that is evaluated inside the logistic (aka *sigmoid*) function to return a value between 0 and 1 (a probability). This is, we build a rule to predict the probability that the outcome is one of the form\n",
        "\n",
        "$$\\widehat{\\text{Pr}(Y = 1|X)} = \\sigma(b_0 + b_1 X_1 + ... + b_p X_p)$$\n",
        "\n",
        "where $\\sigma(\\cdot)$ is the sigmoid function\n",
        "\n",
        "$$\\sigma(u) = \\frac{1}{1 + \\exp(-u)} = \\frac{\\exp(u)}{1 + \\exp(u)}.$$\n",
        "\n",
        "With the predicted probabilities in hand, we can then *classify* the outcome as either a 1 or 0 depending on whether the predicted probability is greater than 0.5.\n"
      ],
      "metadata": {
        "id": "G0rrPvri7Cmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set data up for use with logistic regression\n",
        "\n",
        "# Select features and target variable for training\n",
        "features = ['tenure', 'monthly_charges', 'total_charges', 'contract',\n",
        "            'payment_method', 'topic', 'polarity', 'sentiment']\n",
        "\n",
        "# Get outcome and feature data in train and validation data including dummies\n",
        "# for our categorical variables\n",
        "# Because we are not doing regularization or any kind of variable selection,\n",
        "# we will drop one of each set of dummies.\n",
        "X_train = pd.get_dummies(train_data[features], dtype=float, drop_first=True)\n",
        "y_train = train_data['churn']\n",
        "\n",
        "X_validation = pd.get_dummies(validation_data[features], dtype=float, drop_first=True)\n",
        "y_validation = validation_data['churn']\n",
        "\n",
        "# Align validation data to ensure same columns as training data\n",
        "X_validation = X_validation.reindex(columns=X_train.columns, fill_value=0)\n",
        "\n",
        "# Fitting the logistic regression\n",
        "logistic_model = LogisticRegression(max_iter=1000, random_state=726, penalty = None)\n",
        "logistic_model.fit(X_train, y_train)\n",
        "\n",
        "# Estimated model parameters\n",
        "print(pd.DataFrame(data={'Variable Names': X_train.columns, 'Coefficient': logistic_model.coef_[0]}))\n",
        "print('\\n')\n",
        "print('Intercept', logistic_model.intercept_[0])"
      ],
      "metadata": {
        "id": "yL53shdv-ZjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how we do predicting churn in the validation data."
      ],
      "metadata": {
        "id": "wT-xL8UPDRZg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the validation set\n",
        "y_pred_logistic = logistic_model.predict(X_validation)\n",
        "y_pred_prob_logistic = logistic_model.predict_proba(X_validation)[:, 1]"
      ],
      "metadata": {
        "id": "yKVitoIjE0Xr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Confusion Matrix\n",
        "\n",
        "A standard way to look at predictive performance for classification is via a *confusion matrix*. The confusion matrix just represents classification accuracy by displaying *True Positives* (TP), *False Positives* (FP), *True Negatives* (TN), and *False Negatives* (FN) in the binary case. (With multiple classes, the confusion matrix shows true classifications into each class and false classifications into each class.)"
      ],
      "metadata": {
        "id": "tkKbapzsLb7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display confusion matrix\n",
        "ConfusionMatrixDisplay.from_predictions(y_validation, y_pred_logistic)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "p-bsz6ysDUot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In our toy example, we see that we do a pretty good job predicting our 200 hold observations. We make 10 total mistakes, 5 FP and 5 FN."
      ],
      "metadata": {
        "id": "P4xJveVEMu5n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Other common performance measures.\n",
        "\n",
        "There are many other performance measures people look at. Here we compute a few more:\n",
        "\n",
        "\n",
        "\n",
        "*   Precision = TP/(TP + FP). Precision is the ratio of true positives to overall positives (total number of times the class was predicted). Here we're really focusing on false positives: Precision will be high when there are few false positives. You would care about this metric in situations where false positives are relatively costly. E.g. spam detection, fraud detection.\n",
        "*   Recall = TP/(TP + FN). Recall, also called *sensitivity* is the ratio of true positive predictions to actual positive cases. Here we're really focusing on false negatives: Recall will be high when there are few false negatives. You would care about this metric in situations where false negatives are relatively costly. E.g. medical screening, churn prediction.\n",
        "*   F1 = 2TP/(2TP + FP + FN). F1 tries to balance false negatives and false positives.\n",
        "*   Accuracy. (TP + TN)/N. Fraction (sometimes reported as number) of correct predictions."
      ],
      "metadata": {
        "id": "pQeowByBM5c3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute classification metrics for the logistic regression model\n",
        "logistic_classification_metrics = classification_report(y_validation, y_pred_logistic, output_dict=True)\n",
        "pd.DataFrame(logistic_classification_metrics)"
      ],
      "metadata": {
        "id": "CH6VwhQgDQ9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In our toy example, we end up with identical precision, recall, and f1. This is a fluke. It just happens that the true and false predictions line up in exactly the right way.\n",
        "\n",
        "In the report, we have two additional columns *macro avg* and *weighted avg*.\n",
        "\n",
        "\n",
        "\n",
        "*   macro avg: Reports the average of the associated performance measure across classes. E.g. in our example, macro avg for precision = (0.95098+0.94898)/2 (where 2 is the number of classes we are averaging over).\n",
        "*   weighted avg: Reports the average of the associated performance measure across classes weighted by class size. E.g. in our example, weighted avg for precision = (102/200)$*$0.95098+(98/200)$*$0.94898\n",
        "\n"
      ],
      "metadata": {
        "id": "RTTK-HGnSqpO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ROC Curve and AUC\n",
        "\n",
        "The *ROC curve* is another commonly provided summary of prediction accuracy. (ROC stands for *receiver operating characteristic* which is not particularly evocative as a name).\n",
        "\n",
        "ROC plots the TP rate = TP/(TP+FP) against the FP rate FP/(TP + FP) where TP and FP are calculated at different \"decision thresholds\": We consider classifying each observation according to $\\hat{p}_i > \\textrm{decision threshold}$ varying the decision threshold from 0 to 1.\n",
        "\n",
        "The left-hand-side of the curve corresponds to a decision threshold of 0 and the right-hand-side to a decision threshold of 1.\n",
        "\n",
        "The dashed line is what you get from \"random guessing\".\n",
        "\n",
        "*Area under the curve* (AUC) provides a summary of the ROC curve with numbers closer to 1 indicating better performance. An AUC of one would be a perfect ranking of predictions - you always get 100% TP regardless of the threshold."
      ],
      "metadata": {
        "id": "w3L3_JohUbRB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ROC Curve\n",
        "fpr, tpr, thresholds = roc_curve(y_validation, y_pred_prob_logistic)\n",
        "roc_auc_logistic = roc_auc_score(y_validation, y_pred_prob_logistic)\n",
        "\n",
        "plt.plot(fpr, tpr, label=f'Logistic Regression (area = {roc_auc_logistic:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='black', lw=2, linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Vo9-NzsSE49p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cumulative Gain and Lift\n",
        "\n",
        "The last two classification performance summaries we'll look at are *cumulative gain* and *lift*.\n",
        "\n",
        "**Cumulative gain**\n",
        "\n",
        "Cumulative gain considers classification performance when you are interested in targeting x\\% of the \"population\". The cumulative gain chart plots the fraction of individuals to be targeted on the x-axis against the fraction of individuals correctly classified as belonging to the \"1\" group (the true positive rate) on the y-axis.\n",
        "\n",
        "Cumulative gain provides a quick visual of how well our classifier is at targeting our outcome."
      ],
      "metadata": {
        "id": "SdZyIJ87gEGH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a DataFrame with the true values and predicted probabilities\n",
        "data = pd.DataFrame({'true': y_validation, 'prob': y_pred_prob_logistic})\n",
        "data.sort_values(by='prob', ascending=False, inplace=True)\n",
        "\n",
        "# Calculating cumulative gain\n",
        "data['cumulative_gain'] = np.cumsum(data['true']) / data['true'].sum()\n",
        "data['cumulative_percentage'] = np.arange(1, len(data) + 1) / len(data)\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(data['cumulative_percentage'], data['cumulative_gain'], label='Cumulative Gain')\n",
        "plt.plot([0, 1], [0, 1], 'r--', label='Baseline')\n",
        "plt.xlabel('Percentage of samples')\n",
        "plt.ylabel('Cumulative gain')\n",
        "plt.title('Cumulative Gain Chart')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pDf0DoLWn8GE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lift**\n",
        "\n",
        "Lift is essentially just a different representation of cumulative gain. In the lift chart, we are looking at the ratio of results obtained with and without the predictive model.\n",
        "\n",
        "The lift chart is essentially telling us how much more likely we are to see positive responses by targeting people according to our model relative to targeting people at random."
      ],
      "metadata": {
        "id": "3f4jwFl_kUgw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating lift\n",
        "data['lift'] = data['cumulative_gain'] / data['cumulative_percentage']\n",
        "\n",
        "# Plotting lift curve\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(data['cumulative_percentage'], data['lift'], label='Lift Curve')\n",
        "plt.plot([0, 1], [1, 1], 'r--')\n",
        "plt.xlabel('Percentage of samples')\n",
        "plt.ylabel('Lift')\n",
        "plt.title('Lift Curve')\n",
        "plt.legend(loc='upper right')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SBUDjrjbpBAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification Tree\n",
        "\n",
        "Just like we can use trees for regression, we can also use them for classification. The basic ideas are the same."
      ],
      "metadata": {
        "id": "VUTSqXj3J8ez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now let's try a classification tree\n",
        "\n",
        "# Select features and target variable for training\n",
        "features = ['tenure', 'monthly_charges', 'total_charges', 'contract',\n",
        "            'payment_method', 'topic', 'polarity', 'sentiment']\n",
        "\n",
        "# Get outcome and feature data in train and validation data including dummies\n",
        "# for our categorical variables\n",
        "X_train = pd.get_dummies(train_data[features], dtype=float)\n",
        "y_train = train_data['churn']\n",
        "\n",
        "X_validation = pd.get_dummies(validation_data[features], dtype=float)\n",
        "y_validation = validation_data['churn']\n",
        "\n",
        "# Align validation data to ensure same columns as training data\n",
        "X_validation = X_validation.reindex(columns=X_train.columns, fill_value=0)\n",
        "\n",
        "# Cross-validate using only training data to get \"best\" tree\n",
        "cvsplit = KFold(n_splits=5, shuffle=True, random_state=729)\n",
        "\n",
        "# Parameter we want to choose based on cross-validation performance - number of leaves\n",
        "parameters = {'max_leaf_nodes':range(2,51)}\n",
        "\n",
        "# Define model and do cross-validation\n",
        "tree = DecisionTreeClassifier()\n",
        "cv_tree = GridSearchCV(tree, parameters, scoring='accuracy', refit=True, cv=cvsplit)\n",
        "# We can evaluate our performance based on many different measures. We're using\n",
        "# accuracy in this example. The commented line below uses recall instead. We might\n",
        "# care more about recall in this example than overall accuracy as false negatives,\n",
        "# incorrectly classifying someone as staying when they are going to leave, may\n",
        "# be more costly than false positives in this example.\n",
        "#cv_tree = GridSearchCV(tree, parameters, scoring='recall', refit=True, cv=cvsplit)\n",
        "\n",
        "# Perform cross validation\n",
        "cv_tree.fit(X_train, y_train)\n",
        "\n",
        "# Pull out and plot the tree corresponding to the best prediction rule\n",
        "# according to CV.\n",
        "best_tree = cv_tree.best_estimator_\n",
        "\n",
        "plot_tree(best_tree, feature_names = X_train.columns)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gvcMIrXbJdo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our tree-based classification rule looks extremely simple. It is clearly very interpretable."
      ],
      "metadata": {
        "id": "HoHE08jznbJS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see what the cross-validated performance looks like with different numbers of leaves."
      ],
      "metadata": {
        "id": "e2mz5kKBmp6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "leaves = cv_tree.cv_results_.get('param_max_leaf_nodes')\n",
        "leaves = leaves.tolist()\n",
        "\n",
        "lranks = cv_tree.cv_results_.get('rank_test_score')\n",
        "loss = cv_tree.cv_results_.get('mean_test_score')\n",
        "\n",
        "plt.plot(leaves, loss, label = 'Loss')\n",
        "plt.axvline(cv_tree.best_params_.get('max_leaf_nodes'),\n",
        "            linestyle=\"--\", color=\"black\", label=\"CV estimate\")\n",
        "plt.xlabel(\"Number of leaves\")\n",
        "plt.ylabel(\"Cross-validation Performance\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "0GE_f6exwrRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification performance of tree\n",
        "\n",
        "Let's see how well the tree does in classifying our held out observations."
      ],
      "metadata": {
        "id": "H4qR3Js0mztn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the validation set\n",
        "y_pred_tree = best_tree.predict(X_validation)\n",
        "y_pred_prob_tree = best_tree.predict_proba(X_validation)[:, 1]"
      ],
      "metadata": {
        "id": "f-uFkMN-zf6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display confusion matrix\n",
        "ConfusionMatrixDisplay.from_predictions(y_validation, y_pred_tree)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "i4Jjjn_W0mli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is identical to what we saw for logistic regression. It then follows that recall, precision, and f1 will also be the same."
      ],
      "metadata": {
        "id": "_dY0V2pGm-Qd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute classification metrics for the tree regression model\n",
        "tree_classification_metrics = classification_report(y_validation, y_pred_tree, output_dict=True)\n",
        "pd.DataFrame(tree_classification_metrics)"
      ],
      "metadata": {
        "id": "wG1cYd3rzqSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can compare the tree model to the logistic model in terms of ROC and lift as well."
      ],
      "metadata": {
        "id": "NykCYMOXnG25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ROC Curve\n",
        "fprtree, tprtree, thresholdstree = roc_curve(y_validation, y_pred_prob_tree)\n",
        "roc_auc_tree = roc_auc_score(y_validation, y_pred_prob_tree)\n",
        "\n",
        "plt.plot(fpr, tpr, label=f'Logistic Regression (area = {roc_auc_logistic:.2f})')\n",
        "plt.plot(fprtree, tprtree, label=f'Tree (area = {roc_auc_tree:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='black', lw=2, linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "NPio-ZkM0qAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a DataFrame with the true values and predicted probabilities\n",
        "datatr = pd.DataFrame({'true': y_validation, 'prob': y_pred_prob_tree})\n",
        "datatr.sort_values(by='prob', ascending=False, inplace=True)\n",
        "\n",
        "# Calculating cumulative gain\n",
        "datatr['cumulative_gain'] = np.cumsum(datatr['true']) / datatr['true'].sum()\n",
        "datatr['cumulative_percentage'] = np.arange(1, len(datatr) + 1) / len(datatr)\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(data['cumulative_percentage'], data['cumulative_gain'], label='Cumulative Gain - Logistic')\n",
        "plt.plot(datatr['cumulative_percentage'], datatr['cumulative_gain'],\n",
        "         label='Cumulative Gain - Tree')\n",
        "plt.plot([0, 1], [0, 1], 'r--', label='Baseline')\n",
        "plt.xlabel('Percentage of samples')\n",
        "plt.ylabel('Cumulative gain')\n",
        "plt.title('Cumulative Gain Chart')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7k-rZMoVoMb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating lift\n",
        "datatr['lift'] = datatr['cumulative_gain'] / datatr['cumulative_percentage']\n",
        "\n",
        "# Plotting lift curve\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(data['cumulative_percentage'], data['lift'], label='Lift Curve - Logistic')\n",
        "plt.plot(datatr['cumulative_percentage'], datatr['lift'], label='Lift Curve - Tree')\n",
        "plt.plot([0, 1], [1, 1], 'r--')\n",
        "plt.xlabel('Percentage of samples')\n",
        "plt.ylabel('Lift')\n",
        "plt.title('Lift Curve')\n",
        "plt.legend(loc='upper right')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fT8JyX4cpRwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic prediction rule and tree prediction rule give essentially identical performance in our validation data!"
      ],
      "metadata": {
        "id": "SU9DDK1SpbZr"
      }
    }
  ]
}
