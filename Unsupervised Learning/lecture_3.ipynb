{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fengfrankgthb/BUS-41204/blob/main/Unsupervised%20Learning/lecture_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "094QG8Ln-5Bs"
      },
      "source": [
        "Load packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBaMEAMq-5Bu"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "import ipywidgets as widgets\n",
        "from ipywidgets import interact\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Input, Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfb-idjB-5Bu"
      },
      "source": [
        "Load the wine dataset from Scikit-learn's built-in collection, including features and target labels.\n",
        "This dataset contains information on different chemical properties of wines, which will be useful\n",
        "for the clustering and and PCA analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-eAFScr-5Bu"
      },
      "outputs": [],
      "source": [
        "df = load_wine(as_frame=True)\n",
        "df = df.frame\n",
        "df = df.drop(columns=['target'])\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMvr1YZF-5Bv"
      },
      "source": [
        "Before proceeding with the analysis, we scale the data. This step standardizes the range of the\n",
        "features, preventing variables with larger scales from dominating the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fDG2ap2z-5Bv"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "\n",
        "features = scaler.fit(df)\n",
        "features = features.transform(df)\n",
        "\n",
        "# Convert to pandas Dataframe\n",
        "scaled_df = pd.DataFrame(features, columns=df.columns)\n",
        "# Print the scaled data\n",
        "scaled_df.head()\n",
        "X = scaled_df.values\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9Hl8e8h-5Bv"
      },
      "source": [
        "Next, we apply K-means clustering to the scaled data, determining the optimal number of clusters\n",
        "using the elbow method. The elbow method helps identify the point where adding more clusters no\n",
        "longer significantly improves the model’s performance, allowing us to choose the ideal number of\n",
        "clusters for our analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bN9m4fCX-5Bv"
      },
      "outputs": [],
      "source": [
        "wcss = {}\n",
        "for i in range(1, 11):\n",
        "    kmeans = KMeans(n_clusters=i, init=\"k-means++\", random_state=42)\n",
        "    kmeans.fit(X)\n",
        "    wcss[i] = kmeans.inertia_\n",
        "\n",
        "plt.plot(wcss.keys(), wcss.values(), \"gs-\")\n",
        "plt.xlabel(\"Values of 'k'\")\n",
        "plt.ylabel(\"WCSS\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtkxLdTP-5Bv"
      },
      "source": [
        "Based on the elbow method, it appears that three clusters is enough. We will now proceed to apply\n",
        "K-means clustering with three cluster centers to segment the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCrZWrF5-5Bv"
      },
      "outputs": [],
      "source": [
        "kmeans = KMeans(n_clusters=3)\n",
        "kmeans.fit(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSbO68DF-5Bw"
      },
      "source": [
        "Next, we apply Principal Component Analysis (PCA) to reduce the dimensionality of the data,\n",
        "extracting the first two principal components. This allows us to visualize the data in two\n",
        "dimensions while preserving most of the variance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sr1eKlY4-5Bw"
      },
      "outputs": [],
      "source": [
        "pca = PCA(n_components=2)\n",
        "\n",
        "reduced_X = pd.DataFrame(data=pca.fit_transform(X), columns=[\"PC1\", \"PC2\"])\n",
        "centers = pca.transform(kmeans.cluster_centers_)\n",
        "\n",
        "# Reduced Features\n",
        "reduced_X.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6LDNbh0-5Bw"
      },
      "source": [
        "We now visualize the first two principal components, with the data points colored according to their\n",
        "cluster assignments. This visualization helps us understand how the clusters are distributed in the\n",
        "reduced two-dimensional space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AsxkNux0-5Bw"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(7, 5))\n",
        "\n",
        "# Scatter plot\n",
        "plt.scatter(reduced_X[\"PC1\"], reduced_X[\"PC2\"], c=kmeans.labels_)\n",
        "plt.scatter(centers[:, 0], centers[:, 1], marker=\"x\", s=100, c=\"red\")\n",
        "plt.xlabel(\"PC1\")\n",
        "plt.ylabel(\"PC2\")\n",
        "plt.title(\"Wine Cluster\")\n",
        "plt.tight_layout()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlzHZzjL-5Bw"
      },
      "source": [
        "Now, let's take a look at how each feature loads onto the first two principal components.\n",
        "This will show us which features have the biggest impact on the data’s variance in the reduced\n",
        "space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "COGxBXiq-5Bw"
      },
      "outputs": [],
      "source": [
        "component_df = pd.DataFrame(\n",
        "    (pca.components_.T * np.sqrt(pca.explained_variance_)).T,\n",
        "    index=[\"PC1\", \"PC2\"],\n",
        "    columns=df.columns,\n",
        ")\n",
        "# Heat map\n",
        "sns.heatmap(component_df)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlH1UBnM-5Bw"
      },
      "source": [
        "The following interactive scatter plot shows the first two principal components of the wine dataset,\n",
        "with data points color-coded by cluster assignment. You can select different features from the\n",
        "dropdown to visualize how each feature is distributed across these two principal components. This\n",
        "allows us to explore how the features relate to the two principal components."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACCQ3Cqh-5Bw"
      },
      "outputs": [],
      "source": [
        "def plot_scatter(variable):\n",
        "    plt.figure(figsize=(10, 7))\n",
        "\n",
        "    # Scatter plot with cluster colors\n",
        "    scatter = plt.scatter(reduced_X[\"PC1\"], reduced_X[\"PC2\"], c=kmeans.labels_, cmap=\"viridis\", s=30, alpha=0.7)\n",
        "\n",
        "    # Plot cluster centers\n",
        "    plt.scatter(centers[:, 0], centers[:, 1], marker=\"x\", s=150, c=\"red\", label=\"Cluster Centers\")\n",
        "\n",
        "    plt.xlabel(\"PC1\")\n",
        "    plt.ylabel(\"PC2\")\n",
        "    plt.title(\"Wine Cluster with Annotated Labels\")\n",
        "    plt.legend()\n",
        "\n",
        "    # Introduce small random noise to avoid overlap\n",
        "    np.random.seed(42)  # For consistent results\n",
        "    x_offsets = np.random.uniform(-0.2, 0.2, len(df))\n",
        "    y_offsets = np.random.uniform(-0.2, 0.2, len(df))\n",
        "\n",
        "    # Annotate points with the chosen variable\n",
        "    for i, txt in enumerate(df[variable]):\n",
        "        plt.annotate(\n",
        "            txt,\n",
        "            (reduced_X[\"PC1\"][i] + x_offsets[i], reduced_X[\"PC2\"][i] + y_offsets[i]),\n",
        "            fontsize=8,\n",
        "            ha=\"center\", va=\"center\",\n",
        "        )\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Create a dropdown widget for selecting the variable\n",
        "dropdown = widgets.Dropdown(\n",
        "    options=df.columns,\n",
        "    value='alcohol',\n",
        "    description='Variable:',\n",
        "    disabled=False,\n",
        ")\n",
        "\n",
        "# Use interact to create an interactive plot\n",
        "interact(plot_scatter, variable=dropdown)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPx7INos-5Bw"
      },
      "source": [
        "By default, the dropdown above is set to alcohol, which has a strong influence on the second\n",
        "principal component (PC2). You will notice that higher alcohol values tend to cluster at higher PC2\n",
        "values. Similarly, malic acid plays a significant role in the first principal component (PC1). If\n",
        "you select malic_acid from the dropdown, you'll observe that large values of malic acid are\n",
        "primarily found at lower PC1 values. This reflects how the PCA transformation captures variations in\n",
        "the dataset, with PC1 and PC2 separating key features into distinct clusters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEhJQZWZ-5Bw"
      },
      "source": [
        "After exploring the linear dimensionality reduction technique PCA, we now turn to\n",
        "Autoencoders (AEs), a neural network-based approach that generalizes PCA.\n",
        "We begin with the linear Autoencoder, which, when trained with a mean squared error (MSE) loss and\n",
        "linear activation, effectively learns principal components, much like PCA.\n",
        "However, real-world data often exhibits nonlinear structures that PCA cannot capture. This is where\n",
        "nonlinear Autoencoders come in. By introducing activation functions (e.g., sigmoid, relu), AEs can\n",
        "learn complex, hierarchical representations that go beyond simple linear projections.\n",
        "\n",
        "We now first look at a linear AE. A linear AE uses linear activation functions (line 7 and 9)\n",
        "as well as the MSE as a loss function (line 12)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9huakNp-5Bw"
      },
      "outputs": [],
      "source": [
        "# Define input dimensions\n",
        "input_dim_linear = X.shape[1]\n",
        "encoding_dim_linear = 3\n",
        "\n",
        "# Define the autoencoder using Sequential API\n",
        "autoencoder_linear = Sequential([\n",
        "    Dense(encoding_dim_linear, activation='linear', input_shape=(input_dim_linear,)),  # Encoder\n",
        "    Dense(input_dim_linear, activation='linear')  # Decoder\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "autoencoder_linear.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Model summary\n",
        "print(autoencoder_linear.summary())\n",
        "\n",
        "history_linear = autoencoder_linear.fit(X, X,\n",
        "                epochs=200,\n",
        "                batch_size=16,\n",
        "                shuffle=True,\n",
        "                validation_split=0.1,\n",
        "                verbose=0,)\n",
        "\n",
        "# Plot training & validation loss\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(history_linear.history['loss'], label='Train Loss')\n",
        "plt.plot(history_linear.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Linear Autoencoder Loss Curve')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Extract encoded features (encoder part)\n",
        "encoder_linear = Sequential([\n",
        "    autoencoder_linear.layers[0]  # Extract only the encoder layer\n",
        "])\n",
        "\n",
        "encoded_data_linear = encoder_linear.predict(X)\n",
        "\n",
        "# Scatter plot of encoded data\n",
        "plt.figure(figsize=(7, 5))\n",
        "plt.scatter(encoded_data_linear[:, 0], encoded_data_linear[:, 1], c=kmeans.labels_, cmap='viridis', alpha=0.7)\n",
        "plt.xlabel(\"AE1 (Linear)\")\n",
        "plt.ylabel(\"AE2 (Linear)\")\n",
        "plt.title(\"Encoded Feature Space with Clusters (Linear)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFvbDldM-5Bw"
      },
      "source": [
        "Now we do the same but this time with nonlinear activation functions (sigmoid)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePP1KxIw-5Bx"
      },
      "outputs": [],
      "source": [
        "# Define input dimensions\n",
        "input_dim_sigmoid = X.shape[1]\n",
        "encoding_dim_sigmoid = 3\n",
        "\n",
        "# Autoencoder model with sigmoid activation\n",
        "autoencoder_sigmoid = Sequential([\n",
        "    Dense(encoding_dim_sigmoid, activation='sigmoid', input_shape=(input_dim_sigmoid,)),  # Encoder\n",
        "    Dense(input_dim_sigmoid, activation='sigmoid')  # Decoder\n",
        "])\n",
        "\n",
        "\n",
        "# Increase the learning rate (e.g., 0.01)\n",
        "higher_lr = 0.005\n",
        "optimizer_high_lr = Adam(learning_rate=higher_lr)\n",
        "autoencoder_sigmoid.compile(optimizer=optimizer_high_lr, loss='mse')\n",
        "\n",
        "# Model summary\n",
        "print(autoencoder_sigmoid.summary())\n",
        "\n",
        "\n",
        "history_sigmoid = autoencoder_sigmoid.fit(X, X,\n",
        "                epochs=500,\n",
        "                batch_size=16,\n",
        "                shuffle=True,\n",
        "                validation_split=0.1,\n",
        "                verbose=0,)\n",
        "\n",
        "# Plot training & validation loss\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(history_sigmoid.history['loss'], label='Train Loss')\n",
        "plt.plot(history_sigmoid.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Sigmoid Autoencoder Loss Curve')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Encoder model to extract encoded representations\n",
        "encoder_sigmoid = Sequential([\n",
        "    autoencoder_sigmoid.layers[0]  # Extract only the encoder layer\n",
        "])\n",
        "encoded_data_sigmoid = encoder_sigmoid.predict(X)\n",
        "\n",
        "# Scatter plot of encoded data\n",
        "plt.figure(figsize=(7, 5))\n",
        "plt.scatter(encoded_data_sigmoid[:, 0], encoded_data_sigmoid[:, 1], c=kmeans.labels_, cmap='viridis', alpha=0.7)\n",
        "plt.xlabel(\"AE1 (Sigmoid)\")\n",
        "plt.ylabel(\"AE2 (Sigmoid)\")\n",
        "plt.title(\"Encoded Feature Space with Clusters (Sigmoid)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}