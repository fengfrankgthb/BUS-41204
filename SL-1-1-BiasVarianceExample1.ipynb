{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Illustrate Bias/Variance Tradeoff and Validation Exercise Using Financial Asset Prediction Example"
      ],
      "metadata": {
        "id": "vdYLZ_Gl52p3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we will illustrate two concepts for predictive modeling:\n",
        "\n",
        "\n",
        "\n",
        "1.   Bias/Variance tradeoff\n",
        "2.   The role of validation exercises\n",
        "\n",
        "We will do this in a toy example where we try to predict a household's net financial assets using the household income. The data are drawn from the 1991 SIPP and have been used extensively in academic research starting from Poterba, Venti, and Wise (1994) \"401(k) Plans and Tax-Deferred savings.\" We are just using them as a simple, readily accessible set of data to illustrate predictive modeling.\n",
        "\n"
      ],
      "metadata": {
        "id": "xTsO6eut6INC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load some python libraries\n",
        "\n",
        "We are going to do our analysis using Python. We need to make sure that libraries containing the tools we will use are loaded so we can actually make use of them.\n",
        "\n",
        "We will essentialy always use `numpy` and `pandas` (which contain a bunch of useful data manipulation and \"algebra\" tools). We will also generally use functions taken from `scikit-learn` (aka `sklearn` the leading library of basic ML algorithms) and `matplotlib` (plotting functions). `seaborn` has additional data visualization tools that we will often use.\n",
        "\n",
        "In the following code, we import `numpy`, `pandas`, a linear regression function, a basic plotting function, and a KNN regression function (which you can view just as a black-box prediction algorithm - though ask [ChatGPT](https://chatgpt.com/) to explain it to you.)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "a9tB_WFX7-iV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "arG3Z5NF50vy"
      },
      "outputs": [],
      "source": [
        "# Import relevant packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.neighbors import KNeighborsRegressor\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load, examine, and prepare the data\n",
        "\n",
        "We need to load the data. We will do this directly from a github repository for the course."
      ],
      "metadata": {
        "id": "2INYeGcc-vcS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file = \"https://raw.githubusercontent.com/chansen776/MBA-ML-Course-Materials/main/Data/401k.csv\"\n",
        "data = pd.read_csv(file)"
      ],
      "metadata": {
        "id": "4PDIvC7T-2FS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at what's in the data"
      ],
      "metadata": {
        "id": "i89kv_LsB1qK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.describe()"
      ],
      "metadata": {
        "id": "Jqg3k0XLB59Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.columns"
      ],
      "metadata": {
        "id": "0Dzz3fCeDpvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are a lot of variables here. For our exercise, we're only going to use two of them (`inc` = income and `net_tfa` = net total financial assets). Let's look at those two variables."
      ],
      "metadata": {
        "id": "BT2jFxX_CJRK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data[['inc','net_tfa']].describe()"
      ],
      "metadata": {
        "id": "YYaIA38sCI_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have 9915 observations in the data. For our exercise, we're going to subset these into 6000 \"training\" observations and 3915 \"validation\" observations. (More on this later.) We're going to focus initially on the 6000 training observations."
      ],
      "metadata": {
        "id": "rOxyqut_C6zZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the data into one subset of train_size = 6000 observations\n",
        "# with the other subset containing the remaining observations.\n",
        "\n",
        "# We are splitting at random and want results to replicable, so we set the\n",
        "# state of the random number generator. One should generally do this. Of course,\n",
        "# you don't want your results to depend on the specific arbitrary set state, so\n",
        "# it's a good idea to try a few out and make sure results aren't highly sensitive\n",
        "\n",
        "train, test = train_test_split(data, train_size=6000, random_state=7224)\n"
      ],
      "metadata": {
        "id": "f_GI3S32C6Mt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, let's look at a scatter plot of the data we are going to use to build our model."
      ],
      "metadata": {
        "id": "4SVyp_h0GXmc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_net_tfa = sns.scatterplot(x='inc', y='net_tfa', data=train, alpha = 0.5)\n",
        "plot_net_tfa.set(xlabel='Income', ylabel='Net Financial Assets')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BK2wiJU7Gc3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's fit some candidate predictive models\n",
        "\n",
        "1. Sample mean (baseline)\n",
        "2. Linear regression\n",
        "3. KNN with 30 neighbors\n",
        "4. KNN with 1 neighbor"
      ],
      "metadata": {
        "id": "y_KxLk9kL-WX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample mean as prediction rule\n",
        "\n",
        "ytrmean = np.mean(train['net_tfa'])\n",
        "print('Sample mean of net financial assets: {m:=.2f}'.format(m=ytrmean))\n",
        "\n",
        "# Mean squared error\n",
        "mse0 = np.mean((train['net_tfa'] - ytrmean)**2)\n",
        "print('MSE of sample mean: {m:=.2f}'.format(m=mse0))\n",
        "\n",
        "# Root mean squared error\n",
        "rmse0 = np.sqrt(mse0)\n",
        "print('RMSE of sample mean: {m:=.2f}'.format(m=rmse0))\n",
        "\n",
        "# Mean absolute error\n",
        "mae0 = np.mean(np.abs(train['net_tfa'] - ytrmean))\n",
        "print('MAE of sample mean: {m:=.2f}'.format(m=mae0))\n",
        "\n",
        "# Scatter plot with mean line drawn on\n",
        "plot_net_tfa = sns.scatterplot(x='inc', y='net_tfa', data=train, alpha = 0.5)\n",
        "plot_net_tfa.set(xlabel='Income', ylabel='Net Financial Assets')\n",
        "plt.axhline(y=ytrmean, color = 'red')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ZYZe4Y2hMOi2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how linear regression does. Recall that linear regression with one variable is just fitting a prediction rule\n",
        "\n",
        "$$\\widehat{\\texttt{net_tfa}} = b_0 + b_1 \\texttt{income}$$\n",
        "\n",
        "Despite it's simplicity, linear regression is an ML tool and is often very useful."
      ],
      "metadata": {
        "id": "549DbyrgQ7Oi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Linear regression as prediction rule\n",
        "\n",
        "# Fit linear model using training data\n",
        "# Define the model\n",
        "lm_nettfa = LinearRegression()\n",
        "\n",
        "# Fit the model on the training data\n",
        "lm_nettfa.fit(train[['inc']], train['net_tfa'])\n",
        "\n",
        "# Predict on the training data\n",
        "lm_yhat = lm_nettfa.predict(train[['inc']])\n",
        "\n",
        "# Mean squared error\n",
        "mselm = np.mean((train['net_tfa'] - lm_yhat)**2)\n",
        "print('MSE of linear model: {m:=.2f}'.format(m=mselm))\n",
        "\n",
        "# Root mean squared error\n",
        "rmselm = np.sqrt(mselm)\n",
        "print('RMSE of linear model: {m:=.2f}'.format(m=rmselm))\n",
        "\n",
        "# Mean absolute error\n",
        "maelm = np.mean(np.abs(train['net_tfa'] - lm_yhat))\n",
        "print('MAE of linear model: {m:=.2f}'.format(m=maelm))\n",
        "\n",
        "# R^2 relative to baseline\n",
        "r2lm = 1 - (mselm/mse0)\n",
        "print('R^2 of linear model: {m:=.3f}'.format(m=r2lm))\n",
        "\n",
        "# Scatter plot with linear regression model drawn on\n",
        "plot_net_tfa = sns.regplot(x='inc', y='net_tfa', data=train,\n",
        "                           scatter_kws = {'alpha': 0.25},\n",
        "                           line_kws = {'color': 'red'}, ci = None)\n",
        "plot_net_tfa.set(xlabel='Income', ylabel='Net Financial Assets')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hS_q5FnQRbne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try KNN with 30 neighbors"
      ],
      "metadata": {
        "id": "y_xDaNCrW6qu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# KNN regression as prediction rule\n",
        "\n",
        "# Define the model\n",
        "knn30_nettfa = KNeighborsRegressor(n_neighbors=30)\n",
        "\n",
        "# Fit the model on the training data\n",
        "knn30_nettfa.fit(train[['inc']], train['net_tfa'])\n",
        "\n",
        "# Predict on the training data\n",
        "knn30_yhat = knn30_nettfa.predict(train[['inc']])\n",
        "\n",
        "# Create data frame with KNN fits for plotting\n",
        "knn_plot = pd.DataFrame({'net_tfa': train['net_tfa'], 'inc': train['inc'],\n",
        "                         'fits': knn30_yhat})\n",
        "\n",
        "# Mean squared error\n",
        "mseknn30 = np.mean((train['net_tfa'] - knn30_yhat)**2)\n",
        "print('MSE of KNN with 30 neighbors: {m:=.2f}'.format(m=mseknn30))\n",
        "\n",
        "# Root mean squared error\n",
        "rmseknn30 = np.sqrt(mseknn30)\n",
        "print('RMSE of KNN with 30 neighbors: {m:=.2f}'.format(m=rmseknn30))\n",
        "\n",
        "# Mean absolute error\n",
        "maeknn30 = np.mean(np.abs(train['net_tfa'] - knn30_yhat))\n",
        "print('MAE of KNN with 30 neighbors: {m:=.2f}'.format(m=maeknn30))\n",
        "\n",
        "# R^2 relative to baseline\n",
        "r2knn30 = 1 - (mseknn30/mse0)\n",
        "print('R^2 of KNN with 30 neighbors: {m:=.3f}'.format(m=r2knn30))\n",
        "\n",
        "# Scatter plot with KNN fit drawn on\n",
        "plot_knn_net_tfa = sns.scatterplot(x='inc', y='net_tfa', data=train,\n",
        "                                   alpha = 0.5)\n",
        "plot_knn_net_tfa.set(xlabel='Income', ylabel='Net Financial Assets')\n",
        "sns.lineplot(x='inc', y='fits', data=knn_plot, color='red')  # k-NN fit line\n"
      ],
      "metadata": {
        "id": "2KzPcDTwW6Hb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As our final example, let's look at KNN with 1 neighbor."
      ],
      "metadata": {
        "id": "8nlWJuAuaeON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "knn1_nettfa = KNeighborsRegressor(n_neighbors=1)\n",
        "\n",
        "# Fit the model on the training data\n",
        "knn1_nettfa.fit(train[['inc']], train['net_tfa'])\n",
        "\n",
        "# Predict on the training data\n",
        "knn1_yhat = knn1_nettfa.predict(train[['inc']])\n",
        "\n",
        "# Create data frame with KNN fits for plotting\n",
        "knn_plot = pd.DataFrame({'net_tfa': train['net_tfa'], 'inc': train['inc'],\n",
        "                         'fits': knn1_yhat})\n",
        "\n",
        "# Mean squared error\n",
        "mseknn1 = np.mean((train['net_tfa'] - knn1_yhat)**2)\n",
        "print('MSE of KNN with 1 neighbor: {m:=.2f}'.format(m=mseknn1))\n",
        "\n",
        "# Root mean squared error\n",
        "rmseknn1 = np.sqrt(mseknn1)\n",
        "print('RMSE of KNN with 1 neighbor: {m:=.2f}'.format(m=rmseknn1))\n",
        "\n",
        "# Mean absolute error\n",
        "maeknn1 = np.mean(np.abs(train['net_tfa'] - knn1_yhat))\n",
        "print('MAE of KNN with 1 neighbor: {m:=.2f}'.format(m=maeknn1))\n",
        "\n",
        "# R^2 relative to baseline\n",
        "r2knn1 = 1 - (mseknn1/mse0)\n",
        "print('R^2 of KNN with 1 neighbor: {m:=.3f}'.format(m=r2knn1))\n",
        "\n",
        "# Scatter plot with KNN fit drawn on\n",
        "plot_knn_net_tfa = sns.scatterplot(x='inc', y='net_tfa', data=train,\n",
        "                                   alpha = 0.5)\n",
        "plot_knn_net_tfa.set(xlabel='Income', ylabel='Net Financial Assets')\n",
        "sns.lineplot(x='inc', y='fits', data=knn_plot, color='red')  # k-NN fit line"
      ],
      "metadata": {
        "id": "YdCYvS5WahWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "\n",
        "As we allow the prediction rule to become more \"complex,\" we capture the data used to learn the prediction rule better.\n",
        "\n",
        "With a continuous input, we can find lots of rules that memorize the data. That is, perfectly predict each outcome and have 0 loss **in the sample that was used to learn the prediction rule!**\n",
        "\n",
        "# Question:\n",
        "\n",
        "How do we decide whether the rule we have learned generalizes to predict **new** observations?\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2_CiY7SzdOud"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Validation exercise\n",
        "\n",
        "We actually try out our learned rules to predict new observations!"
      ],
      "metadata": {
        "id": "hSBPtduDI1C0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have already fit our candidate prediction rules. Let's see how they do in\n",
        "the test data that was not included as part of the training/learning/fitting\n",
        "process.\n",
        "\n"
      ],
      "metadata": {
        "id": "AHkawymaJzYE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample mean as prediction rule\n",
        "\n",
        "# Validation mean squared error\n",
        "Vmse0 = np.mean((test['net_tfa'] - ytrmean)**2)\n",
        "print('Validation MSE of sample mean: {m:=.2f}'.format(m=Vmse0))\n",
        "\n",
        "# Validation root mean squared error\n",
        "Vrmse0 = np.sqrt(Vmse0)\n",
        "print('Validation RMSE of sample mean: {m:=.2f}'.format(m=Vrmse0))\n",
        "\n",
        "# Validation mean absolute error\n",
        "Vmae0 = np.mean(np.abs(test['net_tfa'] - ytrmean))\n",
        "print('Validation MAE of sample mean: {m:=.2f}'.format(m=Vmae0))\n"
      ],
      "metadata": {
        "id": "aiD7FmKkJCDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Linear model as prediction rule\n",
        "\n",
        "# Predict on the validation data\n",
        "lm_yhat = lm_nettfa.predict(test[['inc']])\n",
        "\n",
        "# Validation mean squared error\n",
        "Vmselm = np.mean((test['net_tfa'] - lm_yhat)**2)\n",
        "print('Validation MSE of linear model: {m:=.2f}'.format(m=Vmselm))\n",
        "\n",
        "# Validation root mean squared error\n",
        "Vrmselm = np.sqrt(Vmselm)\n",
        "print('Validation RMSE of linear model: {m:=.2f}'.format(m=Vrmselm))\n",
        "\n",
        "# Validation mean absolute error\n",
        "Vmaelm = np.mean(np.abs(test['net_tfa'] - lm_yhat))\n",
        "print('Validation MAE of linear model: {m:=.2f}'.format(m=Vmaelm))\n",
        "\n",
        "# Validation R^2 relative to baseline\n",
        "Vr2lm = 1 - (Vmselm/Vmse0)\n",
        "print('Validation R^2 of linear model: {m:=.3f}'.format(m=Vr2lm))"
      ],
      "metadata": {
        "id": "6gDbQ3wyJ2LX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# KNN regression as prediction rule\n",
        "\n",
        "# Predict on the validation data\n",
        "knn30_yhat = knn30_nettfa.predict(test[['inc']])\n",
        "\n",
        "# Validation mean squared error\n",
        "Vmseknn30 = np.mean((test['net_tfa'] - knn30_yhat)**2)\n",
        "print('Validation MSE of KNN with 30 neighbors: {m:=.2f}'.format(m=Vmseknn30))\n",
        "\n",
        "# Validation root mean squared error\n",
        "Vrmseknn30 = np.sqrt(Vmseknn30)\n",
        "print('Validation RMSE of KNN with 30 neighbors: {m:=.2f}'.format(m=Vrmseknn30))\n",
        "\n",
        "# Validation mean absolute error\n",
        "Vmaeknn30 = np.mean(np.abs(test['net_tfa'] - knn30_yhat))\n",
        "print('Validation MAE of KNN with 30 neighbors: {m:=.2f}'.format(m=Vmaeknn30))\n",
        "\n",
        "# Validation R^2 relative to baseline\n",
        "Vr2knn30 = 1 - (Vmseknn30/Vmse0)\n",
        "print('Validation R^2 of KNN with 30 neighbors: {m:=.3f}'.format(m=Vr2knn30))"
      ],
      "metadata": {
        "id": "2bUoqMVGK9rI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the validation data\n",
        "knn1_yhat = knn1_nettfa.predict(test[['inc']])\n",
        "\n",
        "# Validation mean squared error\n",
        "Vmseknn1 = np.mean((test['net_tfa'] - knn1_yhat)**2)\n",
        "print('Validation MSE of KNN with 1 neighbor: {m:=.2f}'.format(m=Vmseknn1))\n",
        "\n",
        "# Validation root mean squared error\n",
        "Vrmseknn1 = np.sqrt(Vmseknn1)\n",
        "print('Validation RMSE of KNN with 1 neighbor: {m:=.2f}'.format(m=Vrmseknn1))\n",
        "\n",
        "# Validation mean absolute error\n",
        "Vmaeknn1 = np.mean(np.abs(test['net_tfa'] - knn1_yhat))\n",
        "print('Validation MAE of KNN with 1 neighbor: {m:=.2f}'.format(m=Vmaeknn1))\n",
        "\n",
        "# Validation R^2 relative to baseline\n",
        "Vr2knn1 = 1 - (Vmseknn1/Vmse0)\n",
        "print('Validation R^2 of KNN with 1 neighbor: {m:=.3f}'.format(m=Vr2knn1))"
      ],
      "metadata": {
        "id": "iNcKqga_Mc6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Aside: Stability\n",
        "\n",
        "We should probably make sure our results are not driven by an \"unfortunate\" train/test split. Let's try a few more times with random splits.\n",
        "\n",
        "Because we are making random splits, we will get different results each time we run the following code."
      ],
      "metadata": {
        "id": "EGjAOSLNNrAn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start by defining a function so we don't have to retype the same things over and over."
      ],
      "metadata": {
        "id": "-paKw3iTOG2F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_robustness(data):\n",
        "\n",
        "  # Create a new train and test split with a random seed\n",
        "  train, test = train_test_split(data, train_size=6000)\n",
        "\n",
        "  # Sample mean as prediction rule\n",
        "  m0 = np.mean(train['net_tfa'])\n",
        "\n",
        "  # Linear model as prediction rule\n",
        "  lm0 = LinearRegression()\n",
        "  lm0.fit(train[['inc']], train['net_tfa'])\n",
        "\n",
        "  # KNN regression with 30 neighbors as prediction rule\n",
        "  knn30 = KNeighborsRegressor(n_neighbors=30)\n",
        "  knn30.fit(train[['inc']], train['net_tfa'])\n",
        "\n",
        "  # KNN regression with 1 neighbor as prediction rule\n",
        "  knn1 = KNeighborsRegressor(n_neighbors=1)\n",
        "  knn1.fit(train[['inc']], train['net_tfa'])\n",
        "\n",
        "  # Evaluate test data prediction performance\n",
        "  rmse = [np.sqrt(np.mean((test['net_tfa'] - m0)**2)) ,\n",
        "          np.sqrt(np.mean((test['net_tfa'] - lm0.predict(test[['inc']]))**2)) ,\n",
        "          np.sqrt(np.mean((test['net_tfa'] - knn30.predict(test[['inc']]))**2)),\n",
        "          np.sqrt(np.mean((test['net_tfa'] - knn1.predict(test[['inc']]))**2))]\n",
        "\n",
        "  mae = [np.mean(np.abs(test['net_tfa'] - m0)) ,\n",
        "         np.mean(np.abs(test['net_tfa'] - lm0.predict(test[['inc']]))) ,\n",
        "         np.mean(np.abs(test['net_tfa'] - knn30.predict(test[['inc']]))) ,\n",
        "         np.mean(np.abs(test['net_tfa'] - knn1.predict(test[['inc']]))) ]\n",
        "\n",
        "  R2 = 1-(np.array(rmse)**2/np.mean((test['net_tfa'] - m0)**2))\n",
        "\n",
        "  return rmse, mae, R2\n",
        "\n"
      ],
      "metadata": {
        "id": "FXTgSr6HN-cG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's repeat our exercise K = 10 times and look at the results."
      ],
      "metadata": {
        "id": "HD-sCfbvTB4k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rmse = pd.DataFrame(columns=['Mean', 'Linear', 'KNN30', 'KNN1'])\n",
        "mae = pd.DataFrame(columns=['Mean', 'Linear', 'KNN30', 'KNN1'])\n",
        "R2 = pd.DataFrame(columns=['Mean', 'Linear', 'KNN30', 'KNN1'])\n",
        "\n",
        "for k in range(10):\n",
        "  rmsek, maek, R2k = check_robustness(data)\n",
        "  rmse.loc[k] = rmsek\n",
        "  mae.loc[k] = maek\n",
        "  R2.loc[k] = R2k\n"
      ],
      "metadata": {
        "id": "3n7gBfVoTGRm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rmse"
      ],
      "metadata": {
        "id": "9on60cadVi1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mae"
      ],
      "metadata": {
        "id": "X1Z7fnUTaKBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "R2"
      ],
      "metadata": {
        "id": "b1c9f1NzaKuV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Broadly speaking, results are fairly consistent. Based on MSE, we bounce back and forth between linear and KNN30. KNN30 is pretty dominant on MAE."
      ],
      "metadata": {
        "id": "UXPoB4kXaMe9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cross-validation\n",
        "\n",
        "Finally, let's see what happens when we try $K$-fold cross-validation.\n",
        "\n",
        "Recall that cross-validation is another way to structure a validation exercise. We partition the data into $K$ approximately equal sized subsets. We then treat each subset in turn as held out data for a validation exercise, using the remaining subsets to train the prediction rules. We repeat the whole exercise $K$ times, so that each subset of observations gets a turn to be the hold-out set."
      ],
      "metadata": {
        "id": "aXA8cmoudHZY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`sklearn` has built in functions to help with the cross-validation process that we're going to make use of."
      ],
      "metadata": {
        "id": "gl9cHhMceBfL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Packages for K-Fold cross-validation\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "\n",
        "# Let's get back our original train/test split\n",
        "train, test = train_test_split(data, train_size=6000, random_state=7224)\n",
        "\n",
        "# Generate sample partitions for 5-fold cross-validation\n",
        "cv = KFold(n_splits=5, shuffle = True, random_state = 911)\n",
        "\n",
        "# Look at CV performance of sample mean using MSE as the metric\n",
        "# We're tricking the linear regression command to only return an intercept, which\n",
        "# is equivalent to using the sample mean\n",
        "mmse = cross_val_score(LinearRegression(), np.zeros_like(train[['inc']]), train['net_tfa'], scoring='neg_mean_squared_error', cv=cv)\n",
        "print('CV RMSE: Mean')\n",
        "print(np.sqrt(-mmse))\n",
        "# We get NEGATIVE of MSE returned\n",
        "\n",
        "# Look at CV performance of linear model using MSE as the metric\n",
        "lmmse = cross_val_score(LinearRegression(), train[['inc']], train['net_tfa'], scoring='neg_mean_squared_error', cv=cv)\n",
        "print('CV RMSE: Linear')\n",
        "print(np.sqrt(-lmmse))\n",
        "# We get NEGATIVE of MSE returned\n",
        "\n",
        "# Look at CV performance of KNN30 using MSE as the metric\n",
        "knn30mse = cross_val_score(KNeighborsRegressor(n_neighbors=30), train[['inc']], train['net_tfa'], scoring='neg_mean_squared_error', cv=cv)\n",
        "print('CV RMSE: KNN30')\n",
        "print(np.sqrt(-knn30mse))\n",
        "# We get NEGATIVE of MSE returned\n",
        "\n",
        "# Look at CV performance of KNN1 using MSE as the metric\n",
        "knn1mse = cross_val_score(KNeighborsRegressor(n_neighbors=1), train[['inc']], train['net_tfa'], scoring='neg_mean_squared_error', cv=cv)\n",
        "print('CV RMSE: KNN1')\n",
        "print(np.sqrt(-knn1mse))\n",
        "# We get NEGATIVE of MSE returned"
      ],
      "metadata": {
        "id": "j_2JXiKzeO3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's aggregate the performance across all the folds. We get negative MSE as prediction metric above:\n",
        "\n",
        "$$-MSE_k = -\\frac{1}{1200}\\sum_{i \\in \\textrm{Fold} \\ k} (y_i - \\hat{y}_i)^2.$$\n",
        "\n",
        "We get overall MSE by just averaging $MSE_k$:\n",
        "\n",
        "$$MSE = \\frac{1}{5}\\sum_{k=1}^{5} MSE_k.$$\n",
        "\n",
        "We can also calculate overall CV $R^2$ relative to the baseline provided by using the sample mean in the usual way."
      ],
      "metadata": {
        "id": "rrujeo_arYjr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MSE_mean = np.mean(-mmse)\n",
        "MSE_linear = np.mean(-lmmse)\n",
        "MSE_knn30 = np.mean(-knn30mse)\n",
        "MSE_knn1 = np.mean(-knn1mse)\n",
        "\n",
        "print('RMSE mean: {m:=.2f}'.format(m=np.sqrt(MSE_mean)))\n",
        "print('RMSE linear: {m:=.2f}'.format(m=np.sqrt(MSE_linear)))\n",
        "print('RMSE knn30: {m:=.2f}'.format(m=np.sqrt(MSE_knn30)))\n",
        "print('RMSE knn1: {m:=.2f}'.format(m=np.sqrt(MSE_knn1)))\n",
        "\n",
        "R2_mean = 1-(MSE_mean/MSE_mean)\n",
        "R2_linear = 1-(MSE_linear/MSE_mean)\n",
        "R2_knn30 = 1-(MSE_knn30/MSE_mean)\n",
        "R2_knn1 = 1-(MSE_knn1/MSE_mean)\n",
        "\n",
        "print('R2 mean: {m:=.3f}'.format(m=R2_mean))\n",
        "print('R2 linear: {m:=.3f}'.format(m=R2_linear))\n",
        "print('R2 knn30: {m:=.3f}'.format(m=R2_knn30))\n",
        "print('R2 knn1: {m:=.3f}'.format(m=R2_knn1))"
      ],
      "metadata": {
        "id": "10K8yvkhrYFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "More or less aligns with our pure validation exercise. According to MSE, the linear model looks best with KNN30 close.\n",
        "\n",
        "Of course, given that we have a validation data set, we should double check (but we've already done that)."
      ],
      "metadata": {
        "id": "GLjq6OqBuSU0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Aside\n",
        "\n",
        "If we want, we can verify that splits did indeed equally split our 6000 available observations."
      ],
      "metadata": {
        "id": "DDMnKqxyrOfX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, (train_index, test_index) in enumerate(cv.split(train)):\n",
        "    print(f\"Fold {i}:\")\n",
        "    print(f\"  Number of training observations: {len(train_index)}\")\n",
        "    print(f\"  Number of test observations: {len(test_index)}\")\n"
      ],
      "metadata": {
        "id": "NPrNEzDLjnuQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
