{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fengfrankgthb/BUS-41204/blob/main/Reinforcement%20Learning/BanditIllustration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-Armed Bandit Illustration"
      ],
      "metadata": {
        "id": "t1zBbwaduJLs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we will illustrate simple algorithms for multi-armed bandits in a canonical (made-up) example.\n",
        "\n",
        "We will consider a scenario where we have 10 different options we need to choose between. The payoff for each option at each time step will be generated by a normal random variable with variance 1 and a different mean reward. We will consider performance by making 1000 sequential choices among the actions.\n",
        "\n",
        "We will consider the performance of different algorithms in this setting by simulating performance across 2000 different settings where the mean reward for each action in each setting is a different (randomly) generated mean reward.  "
      ],
      "metadata": {
        "id": "5fhparZSuMzk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Python Libraries"
      ],
      "metadata": {
        "id": "-0egrEV3vYps"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As usual, we'll start by importing the libraries we will use."
      ],
      "metadata": {
        "id": "RkDr81XYvbWN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "6A9hBSWTvfHt"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions"
      ],
      "metadata": {
        "id": "B50HGlUSvg7F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now define the functions that make up our simulation.\n",
        "\n",
        "\n",
        "\n",
        "*   `Bandit`: Creates the reward structure for a given setting\n",
        "*   `EpsilonGreedyAgent`: Defines the $\\epsilon$-greedy algorithm for action choice\n",
        "*   `UCB_Agent`: Defines the upper confidence bound (UCB) algorithm for action choice\n",
        "*   `ThompsonSamplingAgent`: Defines the Thompson Sampling algorithm for action choice\n",
        "*   `run_experiment`: Runs one instance of the experiment\n",
        "*   `plot_results`: A wrapper function that runs all the experiments, tabulates, and displays the results\n",
        "\n",
        "The **$\\epsilon$-greedy algorithm** is a simple random algorithm that\n",
        "\n",
        "\n",
        "*   encourages *exploration* by choosing a random action with probability $\\epsilon$\n",
        "*   encourages *exploitation* by choosing the best current action with probability 1-$\\epsilon$\n",
        "\n",
        "\n",
        "Here, we're using a variant of the **UCB algorithm** that incorporates the estimated variance of each action at each time by choosing the action at time $t+1$ that maximizes\n",
        "\n",
        "\\begin{align*}\n",
        "UCB_j(t) = \\bar{X}_{j,t} + 2\\sqrt{\\frac{s^2_{j,t}\\ln(t)}{N_{j,t}}}\n",
        "\\end{align*}\n",
        "\n",
        "\n",
        "*   $\\bar{X}_{j,t}$ is the average reward of action $j$ at time $t$\n",
        "*   $s^2_{j,t}$ is the variance of the reward of action $j$ at time $t$\n",
        "*   $N_{j,t}$ is the number of times action $j$ has been taken at time $t$\n",
        "\n",
        "**Thompson Sampling** is a more sophisticated probabilistic algorithm based on Bayesian updating. We're including it here as a point of comparison and for fun. If you want to understand what it's doing, paste the code block into your favorite LLM and ask it to explain the code to you. üòÅ\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OYiQ5JmpviY9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Bandit:\n",
        "    \"\"\"Class representing a 10-armed bandit with fixed stochastic rewards\"\"\"\n",
        "    def __init__(self, k=10, steps=1000, seed=None):\n",
        "        # Set random seed for reproducibility\n",
        "        if seed is not None:\n",
        "            np.random.seed(seed)\n",
        "\n",
        "        self.k = k\n",
        "        self.q_true = np.random.normal(0, 1, k)  # True action values\n",
        "        self.optimal_action = np.argmax(self.q_true)\n",
        "\n",
        "        # Precompute all rewards for each action and step\n",
        "        self.rewards = np.random.normal(self.q_true[:, None], 1, (k, steps))\n",
        "\n",
        "    def get_reward(self, action, step):\n",
        "        \"\"\"Returns the precomputed reward for a given action and step\"\"\"\n",
        "        return self.rewards[action, step]\n",
        "\n",
        "class EpsilonGreedyAgent:\n",
        "    \"\"\"Epsilon-Greedy agent that ensures each arm is pulled at least once.\"\"\"\n",
        "    def __init__(self, k=10, epsilon=0.1, initial_draws=False):\n",
        "        self.k = k\n",
        "        self.epsilon = epsilon\n",
        "        self.q_est = np.zeros(k)  # Estimated values\n",
        "        self.action_counts = np.zeros(k)  # Count of times each action is taken\n",
        "        self.initial_draws = initial_draws  # Whether to force each arm to be drawn once\n",
        "        self.initial_pulls = set() if initial_draws else None # Track which actions have been taken\n",
        "\n",
        "    def select_action(self):\n",
        "        \"\"\"Selects an action using epsilon-greedy policy, ensuring each arm is pulled at least once.\"\"\"\n",
        "        # Force each arm to be taken at least once\n",
        "        if self.initial_draws and len(self.initial_pulls) < self.k:\n",
        "            action = len(self.initial_pulls)  # Pick the next untried arm\n",
        "            self.initial_pulls.add(action)\n",
        "            return action\n",
        "\n",
        "        # Follow epsilon-greedy policy after each arm has been pulled at least once\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            return np.random.choice(self.k)\n",
        "        return np.argmax(self.q_est)\n",
        "\n",
        "    def update(self, action, reward):\n",
        "        \"\"\"Updates estimates using incremental formula\"\"\"\n",
        "        self.action_counts[action] += 1\n",
        "        self.q_est[action] += (reward - self.q_est[action]) / self.action_counts[action]\n",
        "\n",
        "class UCB_Agent:\n",
        "    \"\"\"UCB agent requiring at least two pulls per action and incorporating sample variance.\"\"\"\n",
        "\n",
        "    def __init__(self, k=10, c=2):\n",
        "        self.k = k\n",
        "        self.c = c\n",
        "        self.q_est = np.zeros(k)  # Sample means\n",
        "        self.action_counts = np.zeros(k, dtype=int)  # Number of pulls per action\n",
        "        self.t = 0  # Time step\n",
        "\n",
        "        # Track rewards for sample variance calculation\n",
        "        self.sum_rewards = np.zeros(k, dtype=np.float64)\n",
        "        self.squared_rewards = np.zeros(k, dtype=np.float64)\n",
        "\n",
        "    def select_action(self):\n",
        "        \"\"\"Selects an action using UCB with sample variance adjustment.\"\"\"\n",
        "        self.t += 1\n",
        "\n",
        "        # Force each action to be taken at least twice before applying UCB formula\n",
        "        if np.min(self.action_counts) < 2:\n",
        "            return np.argmin(self.action_counts)  # Pick the least tried action\n",
        "\n",
        "        # Compute sample means and sample variances\n",
        "        sample_means = self.q_est\n",
        "        sample_variances = np.zeros(self.k)\n",
        "\n",
        "        for action in range(self.k):\n",
        "            if self.action_counts[action] > 1:\n",
        "                mean = self.q_est[action]\n",
        "                squared_mean = self.squared_rewards[action] / self.action_counts[action]\n",
        "                variance = max(squared_mean - mean**2, 1e-6)  # Ensure nonzero variance\n",
        "            else:\n",
        "                variance = 1  # Default to 1 if we don't have enough samples\n",
        "\n",
        "            sample_variances[action] = variance\n",
        "\n",
        "        # Compute UCB values incorporating sample variance\n",
        "        ucb_values = sample_means + self.c * np.sqrt(sample_variances * np.log(self.t) / self.action_counts)\n",
        "\n",
        "        return np.argmax(ucb_values)\n",
        "\n",
        "    def update(self, action, reward):\n",
        "        \"\"\"Updates sample mean and variance estimates.\"\"\"\n",
        "        self.action_counts[action] += 1\n",
        "        n = self.action_counts[action]\n",
        "\n",
        "        self.sum_rewards[action] += reward\n",
        "        self.squared_rewards[action] += reward**2\n",
        "\n",
        "        # Update sample mean\n",
        "        self.q_est[action] += (reward - self.q_est[action]) / n\n",
        "\n",
        "class ThompsonSamplingAgent:\n",
        "    \"\"\"Thompson Sampling agent using Normal-Inverse-Gamma (NIG) conjugate priors.\"\"\"\n",
        "\n",
        "    def __init__(self, k=10, mu_0=0, kappa_0=1, alpha_0=1, beta_0=1):\n",
        "        self.k = k  # Number of arms\n",
        "\n",
        "        # Prior parameters\n",
        "        self.mu_0 = np.full(k, mu_0, dtype=np.float64)  # Mean prior\n",
        "        self.kappa_0 = kappa_0  # Strength of prior on mean\n",
        "        self.alpha_0 = np.full(k, alpha_0, dtype=np.float64)  # Shape parameter (prior)\n",
        "        self.beta_0 = np.full(k, beta_0, dtype=np.float64)  # Scale parameter (prior)\n",
        "\n",
        "        # Posterior parameters (initialize with priors)\n",
        "        self.kappa_n = np.full(k, kappa_0, dtype=np.float64)\n",
        "        self.mu_n = np.full(k, mu_0, dtype=np.float64)\n",
        "        self.alpha_n = np.full(k, alpha_0, dtype=np.float64)\n",
        "        self.beta_n = np.full(k, beta_0, dtype=np.float64)\n",
        "\n",
        "        # Track rewards for updating\n",
        "        self.action_counts = np.zeros(k, dtype=int)  # Number of pulls per arm\n",
        "        self.sum_rewards = np.zeros(k, dtype=np.float64)  # Sum of observed rewards\n",
        "        self.squared_rewards = np.zeros(k, dtype=np.float64)  # Sum of squared rewards\n",
        "\n",
        "    def select_action(self):\n",
        "        \"\"\"Selects an action by drawing samples from the posterior distribution.\"\"\"\n",
        "        sampled_means = np.zeros(self.k)\n",
        "\n",
        "        for action in range(self.k):\n",
        "            sampled_variance = 1 / np.random.gamma(self.alpha_n[action], 1 / self.beta_n[action])\n",
        "            sampled_means[action] = np.random.normal(self.mu_n[action], np.sqrt(sampled_variance / self.kappa_n[action]))\n",
        "\n",
        "        return np.argmax(sampled_means)\n",
        "\n",
        "    def update(self, action, reward):\n",
        "        \"\"\"Updates the posterior parameters using conjugate Normal-Inverse-Gamma updates.\"\"\"\n",
        "        self.action_counts[action] += 1\n",
        "        n = self.action_counts[action]\n",
        "\n",
        "        self.sum_rewards[action] += reward\n",
        "        self.squared_rewards[action] += reward ** 2\n",
        "\n",
        "        sample_mean = self.sum_rewards[action] / n\n",
        "\n",
        "        # Update parameters\n",
        "        self.kappa_n[action] = self.kappa_0 + n\n",
        "        self.mu_n[action] = (self.kappa_0 * self.mu_0[action] + n * sample_mean) / self.kappa_n[action]\n",
        "        self.alpha_n[action] = self.alpha_0[action] + n / 2\n",
        "\n",
        "        # Compute new beta_n using observed variance\n",
        "        sum_sq_diff = self.squared_rewards[action] - n * sample_mean**2\n",
        "        self.beta_n[action] = self.beta_0[action] + 0.5 * (sum_sq_diff + (self.kappa_0 * n / self.kappa_n[action]) * (sample_mean - self.mu_0[action])**2)\n",
        "\n",
        "def run_experiment(bandit, agent, steps=1000):\n",
        "    rewards = np.zeros(steps)\n",
        "    optimal_action_count = np.zeros(steps)\n",
        "    actions_taken = np.zeros(steps, dtype=int)\n",
        "\n",
        "    for t in range(steps):\n",
        "        action = agent.select_action()\n",
        "        reward = bandit.get_reward(action, t)  # Ensure consistent rewards\n",
        "        agent.update(action, reward)\n",
        "\n",
        "        rewards[t] = reward\n",
        "        actions_taken[t] = action\n",
        "        if action == bandit.optimal_action:\n",
        "            optimal_action_count[t] = 1\n",
        "\n",
        "    if isinstance(agent, ThompsonSamplingAgent):\n",
        "      estimated_q = agent.mu_n\n",
        "    else:\n",
        "      estimated_q = agent.q_est\n",
        "\n",
        "    return rewards, optimal_action_count, actions_taken, estimated_q\n",
        "\n",
        "\n",
        "def plot_results(agents, bandit, steps=1000, runs=2000):\n",
        "    avg_rewards = {name: np.zeros(steps) for name in agents}\n",
        "    optimal_actions = {name: np.zeros(steps) for name in agents}\n",
        "\n",
        "    # Storage for MSE calculations\n",
        "    mse_results = {name: {\"mse_optimal\": [], \"mse_overall\": []} for name in agents}\n",
        "\n",
        "    for _ in range(runs):\n",
        "        new_bandit = Bandit(steps=steps)\n",
        "        true_q = new_bandit.q_true\n",
        "        optimal_action = new_bandit.optimal_action\n",
        "\n",
        "        for name, agent_factory in agents.items():\n",
        "            agent = agent_factory()\n",
        "            rewards, opt_actions, _, estimated_q = run_experiment(new_bandit, agent, steps)\n",
        "\n",
        "            # Store rewards and optimal action selection counts\n",
        "            avg_rewards[name] += rewards\n",
        "            optimal_actions[name] += opt_actions\n",
        "\n",
        "            # Compute MSE\n",
        "            mse_optimal = (estimated_q[optimal_action] - true_q[optimal_action]) ** 2\n",
        "            mse_overall = np.mean((estimated_q - true_q) ** 2)\n",
        "\n",
        "            mse_results[name][\"mse_optimal\"].append(mse_optimal)\n",
        "            mse_results[name][\"mse_overall\"].append(mse_overall)\n",
        "\n",
        "    # Average rewards and optimal action percentages over all runs\n",
        "    for name in agents:\n",
        "        avg_rewards[name] /= runs\n",
        "        optimal_actions[name] /= runs\n",
        "\n",
        "    # Plot Average Rewards\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    for name, rewards in avg_rewards.items():\n",
        "        plt.plot(rewards, label=name)\n",
        "    plt.xlabel(\"Steps\")\n",
        "    plt.ylabel(\"Average Reward\")\n",
        "    plt.legend()\n",
        "    plt.title(\"10-Armed Bandit: Average Reward\")\n",
        "    plt.show()\n",
        "\n",
        "    # Plot Optimal Action Percentage\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    for name, opt_action in optimal_actions.items():\n",
        "        plt.plot(opt_action * 100, label=name)\n",
        "    plt.xlabel(\"Steps\")\n",
        "    plt.ylabel(\"Optimal Action %\")\n",
        "    plt.legend()\n",
        "    plt.title(\"10-Armed Bandit: Optimal Action %\")\n",
        "    plt.show()\n",
        "\n",
        "    # Compute final MSE values\n",
        "    mse_summary = {\"Agent\": [], \"Avg MSE Optimal Action\": [], \"Avg MSE Overall\": []}\n",
        "    for name in agents:\n",
        "        mse_summary[\"Agent\"].append(name)\n",
        "        mse_summary[\"Avg MSE Optimal Action\"].append(np.mean(mse_results[name][\"mse_optimal\"]))\n",
        "        mse_summary[\"Avg MSE Overall\"].append(np.mean(mse_results[name][\"mse_overall\"]))\n",
        "\n",
        "    # Convert to DataFrame for display\n",
        "    df_mse_summary = pd.DataFrame(mse_summary)\n",
        "\n",
        "    print(df_mse_summary)\n"
      ],
      "metadata": {
        "id": "VgNAZAAKExzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiments"
      ],
      "metadata": {
        "id": "QbjWQ7PEuHZl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We start by defining the algorithms we're going to consider:\n",
        "\n",
        "\n",
        "\n",
        "*   **Completely Random** - $\\epsilon$-greedy algorithm with $\\epsilon=1$. A random choice will be taken at each step. This algorithm is equivalent to running a randomized controlled trial with 1000 obervations and probability of each treatment equal to .10.\n",
        "*   **Epsilon-Greedy** - $\\epsilon$-greedy algorithm with $\\epsilon=0.1$. This setting is often used as a default.\n",
        "*   **Completely-Greedy** - $\\epsilon$-greedy algorithm with $\\epsilon=0.1$ Here, we always choose the best option given the current information. We'll see that we tend to get stuck at bad choices because we don't explore enough.\n",
        "*   **UCB** - UCB algorithm as described above\n",
        "*   **ThompsonSampling** - Thompson Sampling (the slowest of the bunch)\n"
      ],
      "metadata": {
        "id": "Zx3fJypWzwWl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize agents\n",
        "agents = {\n",
        "    \"Completely Random (Œµ=1)\": lambda: EpsilonGreedyAgent(epsilon=1),\n",
        "    \"Epsilon-Greedy (Œµ=0.1)\": lambda: EpsilonGreedyAgent(epsilon=0.1),\n",
        "    \"Completely Greedy (Œµ=0)\": lambda: EpsilonGreedyAgent(epsilon=0),\n",
        "    \"UCB (c=2)\": lambda: UCB_Agent(),\n",
        "    \"Thompson Sampling\": lambda: ThompsonSamplingAgent()\n",
        "}\n"
      ],
      "metadata": {
        "id": "Qw9yQM0-IR_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before running the full experiment, we're going to look at the outcome of one scenario."
      ],
      "metadata": {
        "id": "CTia5YkV0tv-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize bandit experiment\n",
        "bandit0 = Bandit(seed = 25)\n",
        "\n",
        "estimated_q_values = {}\n",
        "reward_values = {}\n",
        "opt_action_values = {}\n",
        "\n",
        "# Run the experiment and tabulate results\n",
        "for name, agent_factory in agents.items():\n",
        "    agent = agent_factory()\n",
        "    rewards, opt_actions, a_t, estimated_q = run_experiment(bandit0, agent, steps=1000)\n",
        "    estimated_q_values[name] = estimated_q\n",
        "    reward_values[name] = np.mean(rewards)\n",
        "    opt_action_values[name] = np.sum(opt_actions)\n",
        "\n",
        "df_summary = pd.DataFrame([reward_values, opt_action_values], index=[\"reward_values\", \"opt_action_values\"])\n",
        "display(df_summary)\n",
        "\n",
        "df_results = pd.DataFrame(estimated_q_values)\n",
        "df_results.insert(0, \"True Q\", bandit0.q_true)\n",
        "\n",
        "display(df_results)\n"
      ],
      "metadata": {
        "id": "rkEqRY9z3HXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The table provides the results for this one case. We see the true rewards in the first column and the estimates of the reward for each action obtained from the different algorithms in the remaining columns.\n",
        "\n",
        "We also print the total reward from each algorithm and the number of times the algorithm chooses the best action."
      ],
      "metadata": {
        "id": "AMmMxgmA05AG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now repeat the experiment 2000 times, each with different mean rewards and display the results."
      ],
      "metadata": {
        "id": "mpMUhrU02WT9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the experiment and plot results\n",
        "plot_results(agents, Bandit(seed = 2025), steps=1000, runs=2000)"
      ],
      "metadata": {
        "id": "oE4mbOpYHwz3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}