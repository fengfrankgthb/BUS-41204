{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fengfrankgthb/BUS-41204/blob/main/Reinforcement%20Learning/SimpleBlackjack.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple Reinforcement Learning Example - Blackjack\n",
        "\n",
        "Here, we're going to use Q-learning to \"solve\" infinite deck blackjack. Pretending there are infinite decks means that card counting is irrelevant, which really reduces the state space.\n",
        "\n",
        "We consider a simple action space where the player can only choose to *hit* (get another card) or *stand* (stay with the current total in hand).\n",
        "\n",
        "We also abstract from betting and just try to figure out how to win. Each hand will have three possible outcomes: win (with reward = 1), draw (with reward = 0), and lose (with reward = -1).\n",
        "\n",
        "We will make the environment simple. If the player hits and ends up with a total greater than 21, the player immediately loses. If the player stands before reaching 21, the dealer must play until having cards totalling at least 17. If the dealer draws a card producing a total more than 21 or has a total smaller than the player's total, the player wins. If the dealer and player both have the same total, the game ends in a draw.\n",
        "\n",
        "With these rules in mind, the state space the player needs to keep track of consists of just three things: the sum of the player's cards (12-21), the dealer's card (1-10), and whether the player has a useable ace (aces can count as 1 or 11). This gives only 200 possible states - which is a pretty manageable state space.\n"
      ],
      "metadata": {
        "id": "3KeOmyBLJwVN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Python libraries"
      ],
      "metadata": {
        "id": "O8pKengaLy6G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As usual, we'll start by importing the libraries we'll use."
      ],
      "metadata": {
        "id": "PvgPnuwFL07u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "O2M4FoosV5Ev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions"
      ],
      "metadata": {
        "id": "S1JY8elxJvoZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're now going to define the functions we will need to solve our blackjack game using *Q-learning*.\n",
        "\n",
        "In Reinforcement Learning, A **State** leads to **Action**s. An **Action** creates next **State**s. **Action** has **Quality**. **State** has **Value**. A **State's max Value** is determined by the max quality among all the possible actions after it. An **Action's Quality** is determined by the max value of the State it creates. So the best **Policy** (a chain of actions and states) must be the chain of Actions that bear the max quality among their alternative ones.\n",
        "\n",
        "**The root definitions of state value and action quality**:\n",
        "* In policy $œÄ(a|s)$, the value of a state $s$ is:\n",
        "$$V_œÄ(s) = E_œÄ[G_t|S_t=s]$$\n",
        "where $E_œÄ$ is the expected value of policy $œÄ$ after the an action $a$ at time $t$.\n",
        "* In policy $œÄ(a|s)$, the quality of an action $a$ is:\n",
        "$$Q^œÄ(s,a) = E_œÄ[G_t|S_t=s,A_t=a]$$\n",
        "* $G_t$ is the total discounted return from time step t onwards. It's calculated as the sum of all future rewards, discounted by a factor Œ≥ (gamma) at each step:\n",
        "$$G_t = R_{t+1} + Œ≥R_{t+2} +Œ≥^2R_{t+3} +...=‚àë_{k=0}^‚àû Œ≥^kR_{t+k+1}$$\n",
        "\n",
        "**The state-action value recursive-relation**:\n",
        "* Value of State can be expressed by the probability weighted sum of its subsequent actions:\n",
        "$$V_œÄ(s) = E_œÄ[G_t|S_t=s] = ‚àë_a œÄ(a|s) Q_œÄ(s,a)$$\n",
        "$$ = ‚àë_a œÄ(a|s) ‚àë_{s',r}p(s',r|s,a)[r+ Œ≥V_œÄ(s')]$$\n",
        "* Conversely, quality of an action $a$ at its state $s$ can be expressed as the $p$robability weighted sum of all its subsequent state $s'$\n",
        "$$Q_œÄ(s,a) = ‚àë_{s', r}p(s',r|s,a)[r + Œ≥V_œÄ(s')]$$\n",
        "$$ = ‚àë_{s', r}p(s',r|s,a) r + ‚àë_{s', r}p(s',r|s,a)Œ≥V_œÄ(s')]$$\n",
        "$$ = E[R_{t+1} + Œ≥ Q^œÄ(S_{t+1},A_{t+1})|S_t=s,A_t=a] $$\n",
        "Use the $Q_œÄ(a)$ recuesiverelations to construct Q-learning\n",
        "\n",
        "\n",
        "We start by defining the environment and rules of the game."
      ],
      "metadata": {
        "id": "lcuwlqCPL-E1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wyZpVJVXVskg"
      },
      "outputs": [],
      "source": [
        "class BlackjackEnv: # define the grame environment\n",
        "    def __init__(self):\n",
        "        self.action_space = [0, 1]  # 0 = Stick, 1 = Hit\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Initialize a new round with a random player's hand and dealer's upcard.\"\"\"\n",
        "        self.player_sum = 0\n",
        "        self.usable_ace = False\n",
        "        self.dealer_card = self.draw_card()  # Dealer's first card\n",
        "        self.player_hand = [self.draw_card(), self.draw_card()]\n",
        "        self.player_sum, self.usable_ace = self.evaluate_hand(self.player_hand)\n",
        "        return (self.dealer_card, self.player_sum, self.usable_ace)\n",
        "\n",
        "    def draw_card(self):\n",
        "        \"\"\"Draws a card from an infinite deck (1-10, face cards as 10).\"\"\"\n",
        "        card = random.randint(1, 13)\n",
        "        return min(card, 10)\n",
        "\n",
        "    def evaluate_hand(self, hand):\n",
        "        \"\"\"Calculates hand value and determines if there is a usable ace.\"\"\"\n",
        "        total = sum(hand)\n",
        "        usable_ace = 1 in hand and total + 10 <= 21\n",
        "        if usable_ace:\n",
        "            total += 10\n",
        "        return total, usable_ace\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Performs action in the environment (hit or stick).\"\"\"\n",
        "        if action == 1:  # Hit\n",
        "            self.player_hand.append(self.draw_card())\n",
        "            self.player_sum, self.usable_ace = self.evaluate_hand(self.player_hand)\n",
        "            if self.player_sum > 21:\n",
        "                return (self.dealer_card, self.player_sum, self.usable_ace), -1, True  # Player bust\n",
        "            return (self.dealer_card, self.player_sum, self.usable_ace), 0, False\n",
        "\n",
        "        # Stick: Dealer's turn\n",
        "        dealer_hand = [self.dealer_card, self.draw_card()]\n",
        "        dealer_sum, _ = self.evaluate_hand(dealer_hand)\n",
        "\n",
        "        while dealer_sum < 17:\n",
        "            dealer_hand.append(self.draw_card())\n",
        "            dealer_sum, _ = self.evaluate_hand(dealer_hand)\n",
        "\n",
        "        # Determine reward\n",
        "        if dealer_sum > 21 or self.player_sum > dealer_sum:\n",
        "            return (self.dealer_card, self.player_sum, self.usable_ace, dealer_hand, dealer_sum), 1, True  # Win\n",
        "        elif dealer_sum == self.player_sum:\n",
        "            return (self.dealer_card, self.player_sum, self.usable_ace, dealer_hand, dealer_sum), 0, True  # Draw\n",
        "        else:\n",
        "            return (self.dealer_card, self.player_sum, self.usable_ace, dealer_hand, dealer_sum), -1, True  # Loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For fun - and to check that the environment works correctly - we have a little interactive game where we can play hands of blackjack.\n",
        "\n",
        "To play the game interactively, uncomment the statement\n",
        "\n",
        "`play_blackjack()`\n",
        "\n",
        "or enter it into a new code block."
      ],
      "metadata": {
        "id": "TTY_uNRVMLCF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def play_blackjack():\n",
        "    env = BlackjackEnv()\n",
        "    state = env.reset()\n",
        "    dealer_card, player_sum, usable_ace = state\n",
        "\n",
        "    print(\"\\nWelcome to Blackjack! \")\n",
        "    print(f\"Dealer's visible card: {dealer_card}\")\n",
        "    print(f\"Your hand: {env.player_hand} (Sum: {player_sum}, Usable Ace: {usable_ace})\")\n",
        "\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = input(\"Enter 'h' to Hit or 's' to Stick: \").strip().lower()\n",
        "        if action == 'h':\n",
        "            state, reward, done = env.step(1)\n",
        "            dealer_card, player_sum, usable_ace = state\n",
        "            print(f\"\\nYou drew a card. Your new sum: {player_sum}, Usable Ace: {usable_ace}\")\n",
        "            print(f\"Your hand: {env.player_hand}\")\n",
        "            if done:\n",
        "                print(\"\\nüí• You busted! Dealer wins. üí•\")\n",
        "        elif action == 's':\n",
        "            state, reward, done = env.step(0)\n",
        "            dealer_card, player_sum, usable_ace, dealer_hand, dealer_sum = state\n",
        "            print(\"\\nüõë You chose to stick.\")\n",
        "            print(f\"Dealer's full hand: {dealer_hand} (Final sum: {dealer_sum})\")\n",
        "            done = True\n",
        "\n",
        "        if done:\n",
        "            if reward == 1:\n",
        "                print(\"\\nüéâ You won! Congratulations! üéâ\")\n",
        "            elif reward == 0:\n",
        "                print(\"\\nüòê It's a draw.\")\n",
        "            else:\n",
        "                print(\"\\nüòû You lost. Better luck next time!\")\n",
        "\n",
        "# Uncomment play_blackjack() to actually play hand\n",
        "# Start interactive game\n",
        "# play_blackjack()"
      ],
      "metadata": {
        "id": "up9YXJsyXis-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Above defines the environment and play steps. Running these code will create the learnable data for the RL below.*\n",
        "\n",
        "=============================================================================================\n",
        "\n",
        "The next two functions will be used to visualize the results after we run the learning algorithm. They will allow us to plot the *state-value function* which captures the expected reward of being in any given state and the *optimal action* which is the optimal action for any given state learned by the model."
      ],
      "metadata": {
        "id": "KC2rovGxMgR4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# State-value function and optimal strategy from RL results\n",
        "def compute_state_value_function(q_table):\n",
        "    state_values_usable_ace = np.zeros((10, 10))  # Player sum 12-21, Dealer card 1-10\n",
        "    state_values_no_usable_ace = np.zeros((10, 10))\n",
        "\n",
        "    for dealer_card in range(1, 11):\n",
        "        for player_sum in range(12, 22):  # Player sum range 12-21\n",
        "            for usable_ace in [True, False]:\n",
        "                state = (dealer_card, player_sum, usable_ace)\n",
        "                if state in q_table:\n",
        "                    best_action_value = max(q_table[state])  # Best action's Q-value\n",
        "                    if usable_ace:\n",
        "                        state_values_usable_ace[player_sum - 12, dealer_card - 1] = best_action_value\n",
        "                    else:\n",
        "                        state_values_no_usable_ace[player_sum - 12, dealer_card - 1] = best_action_value\n",
        "\n",
        "    return state_values_usable_ace, state_values_no_usable_ace\n",
        "\n",
        "def compute_optimal_strategy(q_table):\n",
        "    optimal_strategy_usable_ace = np.zeros((10, 10))  # Player sum 12-21, Dealer card 1-10\n",
        "    optimal_strategy_no_usable_ace = np.zeros((10, 10))\n",
        "\n",
        "    for dealer_card in range(1, 11):\n",
        "        for player_sum in range(12, 22):  # Player sum range 12-21\n",
        "            for usable_ace in [True, False]:\n",
        "                state = (dealer_card, player_sum, usable_ace)\n",
        "                if state in q_table:\n",
        "                    best_action = np.argmax(q_table[state])  # 0 = Stick, 1 = Hit\n",
        "                    if usable_ace:\n",
        "                        optimal_strategy_usable_ace[player_sum - 12, dealer_card - 1] = best_action\n",
        "                    else:\n",
        "                        optimal_strategy_no_usable_ace[player_sum - 12, dealer_card - 1] = best_action\n",
        "\n",
        "    return optimal_strategy_usable_ace, optimal_strategy_no_usable_ace"
      ],
      "metadata": {
        "id": "KWk004SQAYbk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The final utility function will just be for plotting the state-value function and optimal action."
      ],
      "metadata": {
        "id": "MnT8_FJONPLu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_blackjack_values(state_values_usable_ace, state_values_no_usable_ace,\n",
        "                          optimal_strategy_usable_ace, optimal_strategy_no_usable_ace):\n",
        "    \"\"\"\n",
        "    Plots the state-value function and optimal strategy for blackjack.\n",
        "\n",
        "    Parameters:\n",
        "    state_values_usable_ace (numpy.ndarray): State-value function with a usable ace.\n",
        "    state_values_no_usable_ace (numpy.ndarray): State-value function without a usable ace.\n",
        "    optimal_strategy_usable_ace (numpy.ndarray): Optimal strategy with a usable ace.\n",
        "    optimal_strategy_no_usable_ace (numpy.ndarray): Optimal strategy without a usable ace.\n",
        "    \"\"\"\n",
        "    # Plot State-Value Function\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "    titles = [\"State-Value Function (Usable Ace)\", \"State-Value Function (No Usable Ace)\"]\n",
        "    data = [state_values_usable_ace, state_values_no_usable_ace]\n",
        "\n",
        "    for ax, title, values in zip(axes, titles, data):\n",
        "        cax = ax.matshow(values, cmap=\"coolwarm\", aspect=\"auto\", origin='lower')\n",
        "        fig.colorbar(cax, ax=ax)\n",
        "        ax.set_xticks(range(10))\n",
        "        ax.set_xticklabels(range(1, 11))\n",
        "        ax.set_yticks(range(10))\n",
        "        ax.set_yticklabels(range(12, 22))\n",
        "        ax.set_xlabel(\"Dealer's Card\", labelpad=10)\n",
        "        ax.set_ylabel(\"Player's Sum\")\n",
        "        ax.xaxis.set_label_position('bottom')\n",
        "        ax.tick_params(axis=\"x\", bottom=True, top=False, labelbottom=True, labeltop=False)\n",
        "        ax.set_title(title)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    # Plot Optimal Strategy\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "    titles = [\"Optimal Strategy (Usable Ace)\", \"Optimal Strategy (No Usable Ace)\"]\n",
        "    data = [optimal_strategy_usable_ace, optimal_strategy_no_usable_ace]\n",
        "\n",
        "    for ax, title, values in zip(axes, titles, data):\n",
        "        cax = ax.matshow(values, cmap=\"coolwarm\", aspect=\"auto\", origin='lower')\n",
        "        fig.colorbar(cax, ax=ax)\n",
        "        ax.set_xticks(range(10))\n",
        "        ax.set_xticklabels(range(1, 11))\n",
        "        ax.set_yticks(range(10))\n",
        "        ax.set_yticklabels(range(12, 22))\n",
        "        ax.set_xlabel(\"Dealer's Card\", labelpad=10)\n",
        "        ax.set_ylabel(\"Player's Sum\")\n",
        "        ax.xaxis.set_label_position('bottom')\n",
        "        ax.tick_params(axis=\"x\", bottom=True, top=False, labelbottom=True, labeltop=False)\n",
        "        ax.set_title(title)\n",
        "\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "TmOQjfsSBIPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we define the Q-learning algorithm.\n",
        "\n",
        "Essentiall, the algorithm works by\n",
        "\n",
        "\n",
        "\n",
        "1.   Making an initial guess for $Q(s,a)$ - the expected reward for taking action $a$ when the state is $s$\n",
        "2.   Repeating the following updating strategy *episodes* number of times:\n",
        "\n",
        "\n",
        "*   Start a hand of blackjack and observe initial state (i.e. see the player's and dealer's hand)\n",
        "*   Choose the action $a \\in \\{hit,stand\\}$ to take using $\\varepsilon$-greedy action choice\n",
        "*   Perform action $a$, receive reward $r$, and see new state $s'$\n",
        "*   Update the Q-value function according to $Q(s,a) = Q(s,a) + \\alpha[r + \\gamma \\max_{a'} Q(s',a') - Q(s,a)]$ for learning rate $\\alpha < 1$ and discount factor $\\gamma < 1$.\n",
        "*   Keep going until hand ends with win/loss/draw.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8313H5WzNZle"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QLearningAgentWithDecay:\n",
        "    def __init__(self, alpha=0.1, gamma=0.9, epsilon_start=1.0, epsilon_min=0.1, episodes=200000):\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.epsilon_start = epsilon_start\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.episodes = episodes\n",
        "        self.q_table = {}  # (dealer_card, player_sum, usable_ace) -> [Q(stick), Q(hit)]\n",
        "        self.win_rates = []  # Store win rates over training\n",
        "\n",
        "        # Calculate decay rate to reach epsilon_min at 3/4 of episodes\n",
        "        self.decay_rate = np.power(epsilon_min / epsilon_start, 1 / (0.75 * episodes))\n",
        "        self.epsilon = epsilon_start\n",
        "\n",
        "    def get_q_values(self, state):\n",
        "        \"\"\"Returns Q-values for a given state, initializing if unseen.\"\"\"\n",
        "        if state not in self.q_table:\n",
        "            self.q_table[state] = [0.0, 0.0]  # Initialize Q-values to neutral exploration\n",
        "        return self.q_table[state]\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        \"\"\"Epsilon-greedy action selection.\"\"\"\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.choice([0, 1])\n",
        "        return np.argmax(self.get_q_values(state))\n",
        "\n",
        "    def update_q(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Performs the Q-learning update.\"\"\"\n",
        "        q_values = self.get_q_values(state)\n",
        "        if done:\n",
        "            q_values[action] += self.alpha * (reward - q_values[action])\n",
        "        else:\n",
        "            next_q_values = self.get_q_values(next_state)\n",
        "            q_values[action] += self.alpha * (reward + self.gamma * max(next_q_values) - q_values[action])\n",
        "\n",
        "    def evaluate(self, env, num_games=1000):\n",
        "        \"\"\"Evaluates agent's win rate over a set number of games.\"\"\"\n",
        "        wins = 0\n",
        "        for _ in range(num_games):\n",
        "            state = env.reset()\n",
        "            done = False\n",
        "            while not done:\n",
        "                action = np.argmax(self.get_q_values(state))  # Always pick best action\n",
        "                state, reward, done = env.step(action)\n",
        "            if reward == 1:\n",
        "                wins += 1\n",
        "        return wins / num_games\n",
        "\n",
        "    def train(self, env, eval_interval=5000):\n",
        "        \"\"\"Trains the agent using Q-learning and tracks win rate.\"\"\"\n",
        "        for episode in range(self.episodes):\n",
        "            state = env.reset()\n",
        "            done = False\n",
        "            while not done:\n",
        "                action = self.choose_action(state)\n",
        "                next_state, reward, done = env.step(action)\n",
        "                self.update_q(state, action, reward, next_state, done)\n",
        "                state = next_state\n",
        "\n",
        "            # Decay epsilon\n",
        "            if episode < 0.75 * self.episodes:\n",
        "                self.epsilon *= self.decay_rate\n",
        "                self.epsilon = max(self.epsilon, self.epsilon_min)\n",
        "\n",
        "            if episode % eval_interval == 0:\n",
        "                win_rate = self.evaluate(env, 1000)\n",
        "                self.win_rates.append(win_rate)\n",
        "                print(f\"Episode {episode}, Win Rate: {win_rate:.2f}, Epsilon: {self.epsilon:.4f}\")\n"
      ],
      "metadata": {
        "id": "prUiag-aeI8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run the algorithm\n",
        "\n",
        "We now run the agent. For the results reported in the notes, we use 4 million episodes. Using this many episodes takes more time than one might want to use in class. It's not horrible though if you want to give it a try."
      ],
      "metadata": {
        "id": "ns-H-RhDO1UW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(2725)\n",
        "\n",
        "# Train the Q-learning agent and track win rates\n",
        "qd2_eval_int = 50000 # For demonstration, set to 1000\n",
        "qd2_ep = 4000000     # For demonstration, set to 10000\n",
        "env = BlackjackEnv()\n",
        "qd2_agent = QLearningAgentWithDecay(alpha=0.001, gamma=0.9, epsilon_min=0.01, episodes=qd2_ep)\n",
        "qd2_agent.train(env, eval_interval=qd2_eval_int)"
      ],
      "metadata": {
        "id": "7ZdYfyVJWO2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's plot the win rate as we move along episodes. We see there's a fair amount of variability, but it looks like the RL player is winning 40-45% of hands. (Note that in casinos [the player wins about 42% of hands](https://www.winstar.com/blog/blackjack-odds/).)"
      ],
      "metadata": {
        "id": "2mIKxfUhPai5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot win rates over training\n",
        "plt.plot(range(0, len(qd2_agent.win_rates) * qd2_eval_int, qd2_eval_int), qd2_agent.win_rates)\n",
        "plt.xlabel(\"Episodes\")\n",
        "plt.ylabel(\"Win Rate\")\n",
        "plt.title(\"Q-learning with Decay Blackjack Win Rate Over Time\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Vpa2QCVbWab3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, let's plot the value function and look at the strategy learned by our RL agent."
      ],
      "metadata": {
        "id": "SrgCEZLpRh8_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the state-value function and optimal strategy\n",
        "qd2_state_values_usable_ace, qd2_state_values_no_usable_ace = compute_state_value_function(qd2_agent.q_table)\n",
        "qd2_optimal_strategy_usable_ace, qd2_optimal_strategy_no_usable_ace = compute_optimal_strategy(qd2_agent.q_table)\n",
        "\n",
        "plot_blackjack_values(qd2_state_values_usable_ace, qd2_state_values_no_usable_ace,\n",
        "                          qd2_optimal_strategy_usable_ace, qd2_optimal_strategy_no_usable_ace)"
      ],
      "metadata": {
        "id": "qLCZIkTxWfcf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the optimal strategy here is basically the classic \"basic\" strategy of Thorp (1966) *Beat the Dealer*."
      ],
      "metadata": {
        "id": "f8-rPUxRRnWW"
      }
    }
  ]
}