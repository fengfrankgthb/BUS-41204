{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-BLIrvVCROf"
      },
      "source": [
        "# Bank Deposit Enrollment Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6osAG7tCZy3"
      },
      "source": [
        "In this notebook, we'll illustrate classification focusing on introducing random forests and boosted trees. We will also look at an \"economically\" motivated evaluation measure.\n",
        "\n",
        "The data are from S. Moro, P. Cortez and P. Rita (2014) “A Data-Driven Approach to Predict the Success of Bank Telemarketing. Decision Support Systems,” Decision Support Systems 62, 22-31.\n",
        "\n",
        "The outcome variable is a binary variable indicating whether a person subscribes to a bank term deposit (y = 1)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AD8BGu8OJ0E"
      },
      "source": [
        "# Python Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-A53Az0yD1EH"
      },
      "source": [
        "As usual, we'll start by importing libraries we're going to make use of."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Bv-y7rz7Aznc",
        "outputId": "9c56e843-ffac-4d79-cbb4-293f819025b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-learn==1.5.2\n",
            "  Downloading scikit_learn-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.2) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.2) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.2) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.2) (3.6.0)\n",
            "Downloading scikit_learn-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.6.1\n",
            "    Uninstalling scikit-learn-1.6.1:\n",
            "      Successfully uninstalled scikit-learn-1.6.1\n",
            "Successfully installed scikit-learn-1.5.2\n",
            "1.5.2\n"
          ]
        }
      ],
      "source": [
        "# Partial dependence plots with binary data result in an error in sklearn 1.6.1\n",
        "# Make sure using an earlier version of sklearn until bug is fixed\n",
        "!pip install scikit-learn==1.5.2\n",
        "\n",
        "import sklearn\n",
        "print(sklearn.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8UB_V0Db39pU"
      },
      "outputs": [],
      "source": [
        "# Import relevant packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "from sklearn.inspection import PartialDependenceDisplay\n",
        "\n",
        "\n",
        "!pip install shap\n",
        "import shap\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IoQKY4dD77u"
      },
      "source": [
        "# Load and examine data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5B1_QbtEEng"
      },
      "source": [
        "We'll import the data from the course github repository."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hOlgq6ub4O8J"
      },
      "outputs": [],
      "source": [
        "# Import data\n",
        "file = \"https://raw.githubusercontent.com/chansen776/MBA-ML-Course-Materials/main/Data/bank-additional-full.csv\"\n",
        "rawdata = pd.read_csv(file, sep = \";\")\n",
        "print(rawdata.shape)\n",
        "print(rawdata.columns)\n",
        "print(rawdata.dtypes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LV93OqsEFYzG"
      },
      "source": [
        "We're going to recode the outcome variable from a string ('yes','no') to a binary (1,0)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6OaLmdZ9g1f"
      },
      "outputs": [],
      "source": [
        "# Recode outcome from \"yes\" and \"no\" to 1 and 0\n",
        "rawdata['y'] = rawdata['y'].replace({'no': 0, 'yes': 1})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_fS7sdvFry0"
      },
      "source": [
        "Note that `duration` is essentially an outcome. You don't know this before the contact and `duration = 0` implies `y = \"no\"`. We're going to drop this variable because we do want to use it as a predictor.\n",
        "\n",
        "**What do you think would happen if we included it?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SVC6nM3m7Gef"
      },
      "outputs": [],
      "source": [
        "# Drop duration column\n",
        "rawdata = rawdata.drop(columns = ['duration'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STPNGn-J5mTL"
      },
      "source": [
        "\n",
        "\n",
        "`pdays = 999` means the person was never contacted before. Otherwise, it is the number of days since the last contact. Let's redefine variables so that they numerically make sense.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3iRPSPS6CrK"
      },
      "outputs": [],
      "source": [
        "# Create variable indicating not previously contacted and replace 999's in pdays with 0's\n",
        "rawdata['never_contacted'] = np.where(rawdata['pdays'] == 999, 1, 0)\n",
        "rawdata['never_contacted'] = rawdata['never_contacted'].astype('category')\n",
        "rawdata['pdays'] = np.where(rawdata['pdays'] == 999, 0, rawdata['pdays'])\n",
        "rawdata['pdays'].describe()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TtywkpzGGKu"
      },
      "source": [
        "Finally, we set aside a hold out data set for validation purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1lNW0yc7RbK"
      },
      "outputs": [],
      "source": [
        "# Split the data into training (80%) and validation (20%) sets\n",
        "train, val = train_test_split(rawdata, test_size = 0.2, random_state = 94)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWt9_VQZORjG"
      },
      "source": [
        "# Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDhcfq-ZGNar"
      },
      "source": [
        "Let's start out by building a logistic regression model to use for predicting customer outcomes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i19V-aR17anZ"
      },
      "outputs": [],
      "source": [
        "# Set data up for use with logistic regression\n",
        "X_train = train.drop(columns = ['y'])\n",
        "y_train = train['y']\n",
        "X_val = val.drop(columns = ['y'])\n",
        "y_val = val['y']\n",
        "\n",
        "# Get dummy variables for our categorical features. Because we are not doing regularization or any kind of variable selection,\n",
        "# we will drop one of each set of dummies.\n",
        "X_train = pd.get_dummies(X_train, drop_first = True)\n",
        "X_val = pd.get_dummies(X_val, drop_first = True)\n",
        "\n",
        "# Align validation data to ensure same columns as training data\n",
        "X_val = X_val.reindex(columns=X_train.columns, fill_value=0)\n",
        "\n",
        "# Fitting the logistic regression\n",
        "logistic_model = LogisticRegression(max_iter=1000, random_state=94, penalty = None)\n",
        "logistic_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the validation set\n",
        "y_pred_logistic = logistic_model.predict(X_val)\n",
        "y_pred_prob_logistic = logistic_model.predict_proba(X_val)[:, 1]\n",
        "\n",
        "# Evaluate the model\n",
        "logistic_classification_metrics = classification_report(y_val, y_pred_logistic, output_dict=True)\n",
        "print(pd.DataFrame(logistic_classification_metrics))\n",
        "\n",
        "ConfusionMatrixDisplay.from_predictions(y_val, y_pred_logistic)\n",
        "plt.show()\n",
        "\n",
        "# ROC Curve\n",
        "fpr, tpr, thresholds = roc_curve(y_val, y_pred_prob_logistic)\n",
        "roc_auc_logistic = roc_auc_score(y_val, y_pred_prob_logistic)\n",
        "\n",
        "plt.plot(fpr, tpr, label=f'Logistic Regression (area = {roc_auc_logistic:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='black', lw=2, linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqidthZuGatP"
      },
      "source": [
        "We get almost 90% accuracy!\n",
        "\n",
        "**How excited should we be?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZbLjVjtOXZF"
      },
      "source": [
        "# Constant Model (Ignore Covariates)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAkkhfWjGnx_"
      },
      "source": [
        "Let's fit a model with no predictors at all. That is we are going to predict the same value for literally everyone in the sample. The predicted probability is just going to be the fraction of individuals in the sample that actually subscribe to a deposit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "216aVNcK-O98"
      },
      "outputs": [],
      "source": [
        "# Constant model\n",
        "y_pred_constant = np.zeros(len(y_val))\n",
        "y_pred_prob_constant = np.zeros(len(y_val))+np.mean(y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "constant_classification_metrics = classification_report(y_val, y_pred_constant, output_dict=True)\n",
        "print(pd.DataFrame(constant_classification_metrics))\n",
        "\n",
        "ConfusionMatrixDisplay.from_predictions(y_val, y_pred_constant)\n",
        "plt.show()\n",
        "\n",
        "# ROC Curve\n",
        "fpr_con, tpr_con, thresholds_con = roc_curve(y_val, y_pred_prob_constant)\n",
        "roc_auc_constant = roc_auc_score(y_val, y_pred_prob_constant)\n",
        "\n",
        "plt.plot(fpr, tpr, label=f'Logistic Regression (area = {roc_auc_logistic:.2f})')\n",
        "plt.plot(fpr_con, tpr_con, label=f'Constant (area = {roc_auc_constant:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='black', lw=2, linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJitakcjG_T3"
      },
      "source": [
        "We get almost the same accuracy as the logistic regression (just under 90%).\n",
        "\n",
        "**Should we be excited?**\n",
        "\n",
        "**What do we care about?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0i_aFRQOf8k"
      },
      "source": [
        "# Random Forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLjvcY8uHrGm"
      },
      "source": [
        "Now let's build a random forest model.\n",
        "\n",
        "A random forest is a prediction rule obtained by averaging many tree prediction rules where each tree prediction rule is trained using a random (sub-)set of the training data. (Jargon: Repeatedly drawing samples from the original data is called a *bootstrap*. Random forests are an example of *bagging* = bootstrap aggregation. L. Breiman (1996) “Bagging predictors” Machine Learning 26, 123-140.)\n",
        "\n",
        "The basic random forest algorithm in *pseudo-code*:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "for b = 1,...,B\n",
        "  1. Generate data y_b,X_b by resampling from the training data\n",
        "  2. Use data y_b, X_b to estimate tree prediction rule tree_b(x)\n",
        "end\n",
        "\n",
        "Random forest prediction rule = rf(x) = (1/B)sum_b tree_b(x)\n",
        "```\n",
        "\n",
        "To estimate the random forest, need to choose number of trees (*B*), any tree tuning parameters, and how many variables to include in each bootstrap sample. Intuition for random forests says we should build many low-bias trees for averaging:\n",
        "\n",
        "*   Choose large *B*\n",
        "*   Make sure trees are \"deep\"\n",
        "\n",
        "Defaults seem to work pretty well.\n",
        "\n",
        "However, here we are going to choose tuning parameters by cross-validation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5YHMZ5HXgII"
      },
      "source": [
        "Tuning parameters were chosen using 5 fold cross-validation by running the following code:\n",
        "\n",
        "```\n",
        "#######################################################\n",
        "# Takes approximately 30 minutes to run online\n",
        "# Best tuning parameters: n_estimators = 1000 , min_samples_leaf = 15\n",
        "\n",
        "# Random forest classifier. Choosing tuning parameters by cross-validation\n",
        "\n",
        "# Set data up for use with random forest\n",
        "X_train = train.drop(columns = ['y'])\n",
        "y_train = train['y']\n",
        "X_val = val.drop(columns = ['y'])\n",
        "y_val = val['y']\n",
        "\n",
        "# Get dummy variables for our categorical features.\n",
        "# We are just going to keep all the dummies\n",
        "X_train = pd.get_dummies(X_train, drop_first = False)\n",
        "X_val = pd.get_dummies(X_val, drop_first = False)\n",
        "\n",
        "# Align validation data to ensure same columns as training data\n",
        "X_val = X_val.reindex(columns=X_train.columns, fill_value=0)\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 250, 500, 1000],\n",
        "    'min_samples_leaf': [1, 15, 30, 60, 120]\n",
        "}\n",
        "\n",
        "# Initialize the RandomForestClassifier\n",
        "rf = RandomForestClassifier(random_state=94)\n",
        "\n",
        "# Initialize GridSearchCV with 5-fold cross-validation\n",
        "cv = KFold(n_splits=5, shuffle=True, random_state=94)\n",
        "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=cv, scoring='neg_log_loss', n_jobs=-1)\n",
        "\n",
        "# Fit the grid search model\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters\n",
        "print(f\"Best parameters: {grid_search.best_params_}\")\n",
        "\n",
        "# Evaluate the best model on the test data\n",
        "best_rf = grid_search.best_estimator_\n",
        "y_pred_rf = best_rf.predict(X_val)\n",
        "y_pred_prob_rf = best_rf.predict_proba(X_val)[:, 1]\n",
        "\n",
        "# Evaluate the model\n",
        "rf_classification_metrics = classification_report(y_val, y_pred_rf, output_dict=True)\n",
        "print(pd.DataFrame(rf_classification_metrics))\n",
        "\n",
        "ConfusionMatrixDisplay.from_predictions(y_val, y_pred_rf)\n",
        "plt.show()\n",
        "\n",
        "# ROC Curve\n",
        "fpr_rf, tpr_rf, thresholds_rf = roc_curve(y_val, y_pred_prob_rf)\n",
        "roc_auc_rf = roc_auc_score(y_val, y_pred_prob_rf)\n",
        "\n",
        "plt.plot(fpr, tpr, label=f'Logistic Regression (area = {roc_auc_logistic:.2f})')\n",
        "plt.plot(fpr_rf, tpr_rf, label=f'Random Forest (area = {roc_auc_rf:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='black', lw=2, linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvZ1lcafNtQ4"
      },
      "source": [
        "To make sure things were stabilized, OOB error across trees was checked with the following code:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "#######################################################\n",
        "# Takes approximately 5 minutes to run online\n",
        "\n",
        "# Initialize lists to store OOB scores and number of trees\n",
        "n_estimators_range = range(1, 1001, 20)  # Number of trees from 1 to 1000\n",
        "oob_scores = []\n",
        "\n",
        "rf_oob = RandomForestClassifier(warm_start=True,\n",
        "        min_samples_leaf=15,\n",
        "        random_state=94,\n",
        "        oob_score=True,  # Enable OOB score\n",
        "        bootstrap=True   # Ensure bootstrap sampling is used\n",
        "    )\n",
        "\n",
        "# Loop through the number of estimators and compute OOB scores\n",
        "for n_estimators in n_estimators_range:\n",
        "    rf_oob.set_params(n_estimators=n_estimators)\n",
        "    rf_oob.fit(X_train, y_train)  # Fit the model\n",
        "    oob_scores.append(rf_oob.oob_score_)  # Collect the OOB score\n",
        "\n",
        "# Plot OOB performance\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(n_estimators_range, oob_scores, label=\"OOB Score\")\n",
        "plt.xlabel(\"Number of Trees\")\n",
        "plt.ylabel(\"Out-of-Bag (OOB) Score\")\n",
        "plt.title(\"OOB Performance vs. Number of Trees in Random Forest\")\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.show()\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCLDbAQJ_IZz"
      },
      "outputs": [],
      "source": [
        "# Set data up for use with random forest\n",
        "X_train = train.drop(columns = ['y'])\n",
        "y_train = train['y']\n",
        "X_val = val.drop(columns = ['y'])\n",
        "y_val = val['y']\n",
        "\n",
        "# Get dummy variables for our categorical features.\n",
        "# We are just going to keep all the dummies\n",
        "X_train = pd.get_dummies(X_train, drop_first = False)\n",
        "X_val = pd.get_dummies(X_val, drop_first = False)\n",
        "\n",
        "# Align validation data to ensure same columns as training data\n",
        "X_val = X_val.reindex(columns=X_train.columns, fill_value=0)\n",
        "\n",
        "# Random forest classifier\n",
        "rf_model = RandomForestClassifier(n_estimators=1000, min_samples_leaf=15, random_state=94)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the validation set\n",
        "y_pred_rf = rf_model.predict(X_val)\n",
        "y_pred_prob_rf = rf_model.predict_proba(X_val)[:, 1]\n",
        "\n",
        "# Evaluate the model\n",
        "rf_classification_metrics = classification_report(y_val, y_pred_rf, output_dict=True)\n",
        "print(pd.DataFrame(rf_classification_metrics))\n",
        "\n",
        "ConfusionMatrixDisplay.from_predictions(y_val, y_pred_rf)\n",
        "plt.show()\n",
        "\n",
        "# ROC Curve\n",
        "fpr_rf, tpr_rf, thresholds_rf = roc_curve(y_val, y_pred_prob_rf)\n",
        "roc_auc_rf = roc_auc_score(y_val, y_pred_prob_rf)\n",
        "\n",
        "plt.plot(fpr, tpr, label=f'Logistic Regression (area = {roc_auc_logistic:.2f})')\n",
        "plt.plot(fpr_rf, tpr_rf, label=f'Random Forest (area = {roc_auc_rf:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='black', lw=2, linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDhE5ZzuJx42"
      },
      "source": [
        "Random forest is mildly more accurate than logistic regression - with a slightly higher AUC.\n",
        "\n",
        "**Is accuracy what we care about?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_RKncjEPKCF"
      },
      "source": [
        "# Boosted Trees"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3msammyO3Zf"
      },
      "source": [
        "Now let's build a boosted tree.\n",
        "\n",
        "A boosted tree is a prediction rule obtained by adding together many tree prediction rules where each tree prediction rule is trained to fit the prediction errors from some previous model. (Boosted trees are an example of *boosting*. R. E. Schapire. The strength of weak learnability. Machine Learning, 5:197–227, 1990.)\n",
        "\n",
        "The basic boosted tree algorithm in *pseudo-code*:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "Initialize residuals as r_i = y_i\n",
        "for b = 1,...,B\n",
        "  1. Fit tree based prediction rule to outcomes r_i to obtain prediction rule f_b(x)\n",
        "  2. Update residuals to r_i = r_i - lambda f_b(x_i)\n",
        "end\n",
        "\n",
        "Boosted tree prediction rule = boost(x) = sum_b lambda f_b(x)\n",
        "```\n",
        "\n",
        "To estimate the boosted tree model, need to choose number of trees (*B*), any tree tuning parameters, and learning rate lambda. Intuition for boosting says we should improve fit at a slow rate:\n",
        "\n",
        "*   Choose lambda small ($\\ll$1)\n",
        "*   Make sure trees are \"shallow\"\n",
        "\n",
        "Here we are going to choose tuning parameters (B,lambda,tree depth) by cross-validation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRhZwh651-Dv"
      },
      "source": [
        "Tuning parameters were chosen using 5 fold cross-validation by running the following code:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "#######################################################\n",
        "# Takes approximately 45 minutes to run online\n",
        "# Best tuning parameters: 'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 60\n",
        "\n",
        "# Gradient Boosting Classifier. Choosing tuning parameters by cross-validation\n",
        "\n",
        "# Set data up for use with GradientBoostingClassifier\n",
        "X_train = train.drop(columns=['y'])\n",
        "y_train = train['y']\n",
        "X_val = val.drop(columns=['y'])\n",
        "y_val = val['y']\n",
        "\n",
        "# Get dummy variables for our categorical features.\n",
        "X_train = pd.get_dummies(X_train, drop_first=False)\n",
        "X_val = pd.get_dummies(X_val, drop_first=False)\n",
        "\n",
        "# Align validation data to ensure same columns as training data\n",
        "X_val = X_val.reindex(columns=X_train.columns, fill_value=0)\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.1, 1],\n",
        "    'max_depth': [2, 3, 4, 5, 6],\n",
        "    'n_estimators': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
        "}\n",
        "\n",
        "# Initialize the GradientBoostingClassifier\n",
        "gbc = GradientBoostingClassifier(random_state=94)\n",
        "\n",
        "# Initialize GridSearchCV with 5-fold cross-validation\n",
        "cv = KFold(n_splits=5, shuffle=True, random_state=94)\n",
        "grid_search = GridSearchCV(estimator=gbc, param_grid=param_grid, cv=cv, scoring='neg_log_loss', n_jobs=-1)\n",
        "\n",
        "# Fit the grid search model\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters\n",
        "print(f\"Best parameters: {grid_search.best_params_}\")\n",
        "\n",
        "# Evaluate the best model on the test data\n",
        "best_gbc = grid_search.best_estimator_\n",
        "y_pred_gbc = best_gbc.predict(X_val)\n",
        "y_pred_prob_gbc = best_gbc.predict_proba(X_val)[:, 1]\n",
        "\n",
        "# Evaluate the model\n",
        "gbc_classification_metrics = classification_report(y_val, y_pred_gbc, output_dict=True)\n",
        "print(pd.DataFrame(gbc_classification_metrics))\n",
        "\n",
        "ConfusionMatrixDisplay.from_predictions(y_val, y_pred_gbc)\n",
        "plt.show()\n",
        "\n",
        "# ROC Curve\n",
        "fpr_gbc, tpr_gbc, thresholds_gbc = roc_curve(y_val, y_pred_prob_gbc)\n",
        "roc_auc_gbc = roc_auc_score(y_val, y_pred_prob_gbc)\n",
        "\n",
        "plt.plot(fpr, tpr, label=f'Logistic Regression (area = {roc_auc_logistic:.2f})')\n",
        "plt.plot(fpr_rf, tpr_rf, label=f'Random Forest (area = {roc_auc_rf:.2f})')\n",
        "plt.plot(fpr_gbc, tpr_gbc, label=f'Gradient Boosting (area = {roc_auc_gbc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='black', lw=2, linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43vzSbepPeod"
      },
      "outputs": [],
      "source": [
        "# Set data up for use with GradientBoostingClassifier\n",
        "X_train = train.drop(columns=['y'])\n",
        "y_train = train['y']\n",
        "X_val = val.drop(columns=['y'])\n",
        "y_val = val['y']\n",
        "\n",
        "# Get dummy variables for our categorical features.\n",
        "X_train = pd.get_dummies(X_train, drop_first=False)\n",
        "X_val = pd.get_dummies(X_val, drop_first=False)\n",
        "\n",
        "# Align validation data to ensure same columns as training data\n",
        "X_val = X_val.reindex(columns=X_train.columns, fill_value=0)\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.1, 1],\n",
        "    'max_depth': [2, 3, 4, 5, 6],\n",
        "    'n_estimators': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
        "}\n",
        "\n",
        "# Initialize the GradientBoostingClassifier\n",
        "gbc_model = GradientBoostingClassifier(random_state=94, learning_rate=0.1, max_depth=4, n_estimators=60)\n",
        "gbc_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "y_pred_gbc = gbc_model.predict(X_val)\n",
        "y_pred_prob_gbc = gbc_model.predict_proba(X_val)[:, 1]\n",
        "\n",
        "# Evaluate the model\n",
        "gbc_classification_metrics = classification_report(y_val, y_pred_gbc, output_dict=True)\n",
        "print(pd.DataFrame(gbc_classification_metrics))\n",
        "\n",
        "ConfusionMatrixDisplay.from_predictions(y_val, y_pred_gbc)\n",
        "plt.show()\n",
        "\n",
        "# ROC Curve\n",
        "fpr_gbc, tpr_gbc, thresholds_gbc = roc_curve(y_val, y_pred_prob_gbc)\n",
        "roc_auc_gbc = roc_auc_score(y_val, y_pred_prob_gbc)\n",
        "\n",
        "plt.plot(fpr, tpr, label=f'Logistic Regression (area = {roc_auc_logistic:.2f})')\n",
        "plt.plot(fpr_rf, tpr_rf, label=f'Random Forest (area = {roc_auc_rf:.2f})')\n",
        "plt.plot(fpr_gbc, tpr_gbc, label=f'Gradient Boosting (area = {roc_auc_gbc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='black', lw=2, linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIjU0rWOQiQ5"
      },
      "source": [
        "Performs about the same as the random forest on this example with these tuning choices."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7G3hEVh628Tn"
      },
      "source": [
        "With boosted trees, we know that after some point, increasing *B* is not going to help and will eventually start to hurt as we overspecialize to the training data (i.e. as we overfit).\n",
        "\n",
        "Rather than use cross-validation to choose tuning parameters (especially *B*), we can use **early stopping** instead. Early stopping monitors prediction performance as we increase *B* and stops as soon as predictive performance remains level/starts to deteriorate.\n",
        "\n",
        "Early stopping can save computation time and simultaneously perform well out-of-sample. It is available with procedures that adapt the model in a step-by-step fashion (which is most of things people actually do)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tM7o9cyK3jpn"
      },
      "outputs": [],
      "source": [
        "# Set data up for use with GradientBoostingClassifier\n",
        "X_train = train.drop(columns=['y'])\n",
        "y_train = train['y']\n",
        "X_val = val.drop(columns=['y'])\n",
        "y_val = val['y']\n",
        "\n",
        "# Get dummy variables for our categorical features.\n",
        "X_train = pd.get_dummies(X_train, drop_first=False)\n",
        "X_val = pd.get_dummies(X_val, drop_first=False)\n",
        "\n",
        "# Align validation data to ensure same columns as training data\n",
        "X_val = X_val.reindex(columns=X_train.columns, fill_value=0)\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.1, 1],\n",
        "    'max_depth': [2, 3, 4, 5, 6]\n",
        "}\n",
        "\n",
        "# Early stopping criteria\n",
        "best_model = None\n",
        "best_score = float('inf')\n",
        "\n",
        "for learning_rate in param_grid['learning_rate']:\n",
        "    for max_depth in param_grid['max_depth']:\n",
        "        # Initialize the GradientBoostingClassifier with early stopping\n",
        "        gbcES = GradientBoostingClassifier(\n",
        "            learning_rate=learning_rate,\n",
        "            max_depth=max_depth,\n",
        "            n_estimators=200,\n",
        "            validation_fraction=0.2,\n",
        "            n_iter_no_change=10,\n",
        "            tol=1e-4,\n",
        "            random_state=94\n",
        "        )\n",
        "\n",
        "        # Fit the model\n",
        "        gbcES.fit(X_train, y_train)\n",
        "\n",
        "        # Evaluate the validation loss from the internal validation split\n",
        "        val_loss = min(gbcES.train_score_[-10:])  # Use the lowest validation loss from early stopping\n",
        "\n",
        "        if val_loss < best_score:\n",
        "            best_score = val_loss\n",
        "            best_model = gbcES\n",
        "\n",
        "# Print the best parameters\n",
        "print(f\"Best model parameters: learning_rate={best_model.learning_rate}, max_depth={best_model.max_depth}\")\n",
        "\n",
        "# Evaluate the best model on the test data\n",
        "y_pred_gbcES = best_model.predict(X_val)\n",
        "y_pred_prob_gbcES = best_model.predict_proba(X_val)[:, 1]\n",
        "\n",
        "# Evaluate the model\n",
        "gbcES_classification_metrics = classification_report(y_val, y_pred_gbcES, output_dict=True)\n",
        "print(pd.DataFrame(gbcES_classification_metrics))\n",
        "\n",
        "ConfusionMatrixDisplay.from_predictions(y_val, y_pred_gbcES)\n",
        "plt.show()\n",
        "\n",
        "# ROC Curve\n",
        "fpr_gbcES, tpr_gbcES, thresholds_gbcES = roc_curve(y_val, y_pred_prob_gbcES)\n",
        "roc_auc_gbcES = roc_auc_score(y_val, y_pred_prob_gbcES)\n",
        "\n",
        "plt.plot(fpr, tpr, label=f'Logistic Regression (area = {roc_auc_logistic:.2f})')\n",
        "plt.plot(fpr_rf, tpr_rf, label=f'Random Forest (area = {roc_auc_rf:.2f})')\n",
        "plt.plot(fpr_gbc, tpr_gbc, label=f'Gradient Boosting (area = {roc_auc_gbc:.2f})')\n",
        "plt.plot(fpr_gbcES, tpr_gbcES, label=f'Gradient Boosting Early (area = {roc_auc_gbcES:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='black', lw=2, linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0261YUfzRkcA"
      },
      "source": [
        "Performance using early stopping is essentially identical to that obtained by full cross-validation in this example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "viIl6HNq9kmP"
      },
      "outputs": [],
      "source": [
        "# Retrain using the best parameters without early stopping\n",
        "long_model = GradientBoostingClassifier(\n",
        "    learning_rate=best_model.learning_rate,\n",
        "    max_depth=best_model.max_depth,\n",
        "    n_estimators=200,\n",
        "    random_state=94\n",
        ")\n",
        "long_model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "train_acc_without = []\n",
        "val_acc_without = []\n",
        "\n",
        "train_acc_with = []\n",
        "val_acc_with = []\n",
        "\n",
        "for i, (train_pred, val_pred) in enumerate(\n",
        "    zip(\n",
        "        long_model.staged_predict(X_train),\n",
        "        long_model.staged_predict(X_val),\n",
        "    )\n",
        "):\n",
        "    train_acc_without.append(accuracy_score(y_train, train_pred))\n",
        "    val_acc_without.append(accuracy_score(y_val, val_pred))\n",
        "\n",
        "for i, (train_pred, val_pred) in enumerate(\n",
        "    zip(\n",
        "        best_model.staged_predict(X_train),\n",
        "        best_model.staged_predict(X_val),\n",
        "    )\n",
        "):\n",
        "    train_acc_with.append(accuracy_score(y_train, train_pred))\n",
        "    val_acc_with.append(accuracy_score(y_val, val_pred))\n",
        "\n",
        "\n",
        "\n",
        "fig, axes = plt.subplots(ncols=2, figsize=(12, 4))\n",
        "\n",
        "axes[0].plot(train_acc_without, label=\"gbm_full\")\n",
        "axes[0].plot(train_acc_with, label=\"gbm_early_stopping\")\n",
        "axes[0].set_xlabel(\"Boosting Iterations\")\n",
        "axes[0].set_ylabel(\"Accuracy (Training)\")\n",
        "axes[0].legend()\n",
        "axes[0].set_title(\"Training Accuracy\")\n",
        "\n",
        "axes[1].plot(val_acc_without, label=\"gbm_full\")\n",
        "axes[1].plot(val_acc_with, label=\"gbm_early_stopping\")\n",
        "axes[1].set_xlabel(\"Boosting Iterations\")\n",
        "axes[1].set_ylabel(\"Accuracy (Validation)\")\n",
        "axes[1].legend()\n",
        "axes[1].set_title(\"Validation Accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUFtSHmuRs4o"
      },
      "source": [
        "Early stopping also saves a lot of computation!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onGSM6q6OkKU"
      },
      "source": [
        "# Evaluate using \"expected profit\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rlH3QzKiF0C"
      },
      "source": [
        "We might want to use an \"economic\" criterion to evaluate our models.\n",
        "\n",
        "If we were thinking about how to value a contact resulting in a new account, we might want to consider\n",
        "\n",
        "\n",
        "\n",
        "1. Initial Deposit and Balance Maintenance over time\n",
        "2. Interest and Fee Income\n",
        "3. Cross-Selling Opportunities\n",
        "4. Customer Retention and Longevity\n",
        "5. Cost of Acquisition and Servicing\n",
        "6. Discount Rate\n",
        "\n",
        "We're not going to consider most of these in our little example.\n",
        "\n",
        "For simplicity, let's suppose that there is a cost $C = 100$ of contacting an individual.\n",
        "\n",
        "Let's suppose that a customer who is contacted and opens an account has the following characteristics:\n",
        "\n",
        "\n",
        "\n",
        "*   Initial deposit = 1000, average maintained balance = 2000\n",
        "*   Assessed fees = 50/year\n",
        "*   Takes out credit card that generates 100 in fees and interest/year\n",
        "*   Stays with the bank for 5 years\n",
        "\n",
        "Let's further assume that the bank\n",
        "\n",
        "\n",
        "\n",
        "*   gets 5%/year in interest from loans made from deposits\n",
        "*   incurs costs of 100 initially and 20/year for account maintenance, setup, ...\n",
        "*   has a discount rate of 5%\n",
        "\n",
        "From this, we have that the net revenue of the customer is (100+50+100-100-20 = 130) in year 1 and (100+50+100-20 = 230) in years 2-5. Lifetime value of the customer is then approximately $R = \\frac{130}{1.05} + \\sum_{j=2}^{5}\\frac{230}{1.05^j} = 904 \\approx 900.$\n",
        "\n",
        "\n",
        "\n",
        "We can then think about the \"expected\" benefit from contacting an individual that we predict will open an account. (Because we are only contacting people we predict will open the account, there are only two outcomes we care about: true positives (TP, people we predict will open and do open) and false positives (FP, people we predict will open but do not). Let's suppose that the TP and FP from our prediction rules tell us what would happen if we contacted new people.\n",
        "\n",
        "The expected benefit is then just\n",
        "\n",
        "$Pr(TP)*(R-C) + Pr(FP)(-C)$\n",
        "\n",
        "We can use our data and prediction rules to fill in estimates of $Pr(TP)$ and $Pr(FP)$ from the different models.\n",
        "\n",
        "In the following, we are going to contact anyone for whom the predicted probability of take-up is greater than 50%.\n",
        "\n",
        "**Are there other things we are abstracting from here that might be important?**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-EpnBXSQqFLH"
      },
      "outputs": [],
      "source": [
        "# Calculate expected profit\n",
        "\n",
        "# Confusion matrix for constant model\n",
        "tn, fp, fn, tp = confusion_matrix(y_val, y_pred_constant).ravel()\n",
        "# Confusion matrix for logistic model\n",
        "tn_log, fp_log, fn_log, tp_log = confusion_matrix(y_val, y_pred_logistic).ravel()\n",
        "# Confusion matrix for random forest model\n",
        "tn_rf, fp_rf, fn_rf, tp_rf = confusion_matrix(y_val, y_pred_rf).ravel()\n",
        "# Confusion matrix for boosting model\n",
        "tn_gbc, fp_gbc, fn_gbc, tp_gbc = confusion_matrix(y_val, y_pred_gbc).ravel()\n",
        "# Confusion matrix for boosting model (early stopping)\n",
        "tn_gbcES, fp_gbcES, fn_gbcES, tp_gbcES = confusion_matrix(y_val, y_pred_gbcES).ravel()\n",
        "\n",
        "# Hypothetical value of customer opening deposit\n",
        "R = 900\n",
        "# Hypothetical \"benefit\" of contacting customer\n",
        "C = -100\n",
        "\n",
        "# Expected profit from constant model\n",
        "N = tn+fp+fn+tp  # Total number of observations\n",
        "Epi_con = (tp/N)*(R+C)+(fp/N)*C\n",
        "# Expected profit from logistic model\n",
        "Epi_log = (tp_log/N)*(R+C)+(fp_log/N)*C\n",
        "# Expected profit from random forest model\n",
        "Epi_rf = (tp_rf/N)*(R+C)+(fp_rf/N)*C\n",
        "# Expected profit from boosting model\n",
        "Epi_gbc = (tp_gbc/N)*(R+C)+(fp_gbc/N)*C\n",
        "# Expected profit from boosting model (early stopping)\n",
        "Epi_gbcES = (tp_gbcES/N)*(R+C)+(fp_gbcES/N)*C\n",
        "\n",
        "\n",
        "print(f\"Expected profit from constant model: {Epi_con}\")\n",
        "print(f\"Expected profit from logistic model: {Epi_log}\")\n",
        "print(f\"Expected profit from random forest model: {Epi_rf}\")\n",
        "print(f\"Expected profit from boosted tree model: {Epi_gbc}\")\n",
        "print(f\"Expected profit from boosted tree model (early stopping): {Epi_gbcES}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGV5raeWvxln"
      },
      "source": [
        "An alternate way to think about evaluating the prediction rules is to consider choosing the threshold for contacting based on the following exercise:\n",
        "\n",
        "\n",
        "\n",
        "1.   First, order all individuals in the sample according to predicted probability of take-up.\n",
        "2.   Consider varying the threshold for contact (or alternatively the fraction of the sample to contact) between 1 (contact no one) and 0 (contact everyone)\n",
        "3.   Choose the model and threshold that gives the highest \"profit.\" (All the true ones give a benefit of R-C, and all the true zeros give a benefit of -C.)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9ii69j52QgT"
      },
      "outputs": [],
      "source": [
        "# Creating a DataFrame with the true values and predicted probabilities\n",
        "datalog = pd.DataFrame({'true': y_val, 'prob': y_pred_prob_logistic})\n",
        "datalog.sort_values(by='prob', ascending=False, inplace=True)\n",
        "\n",
        "datarf = pd.DataFrame({'true': y_val, 'prob': y_pred_prob_rf})\n",
        "datarf.sort_values(by='prob', ascending=False, inplace=True)\n",
        "\n",
        "datagbc = pd.DataFrame({'true': y_val, 'prob': y_pred_prob_gbc})\n",
        "datagbc.sort_values(by='prob', ascending=False, inplace=True)\n",
        "\n",
        "datagbcES = pd.DataFrame({'true': y_val, 'prob': y_pred_prob_gbcES})\n",
        "datagbcES.sort_values(by='prob', ascending=False, inplace=True)\n",
        "\n",
        "# Calculate \"profit\" from contacting people ordered by their predicted probability\n",
        "datalog['profit'] = np.cumsum(datalog['true']*(R-C)+(1-datalog['true'])*C)\n",
        "datalog['cumulative_percentage'] = np.arange(1, len(datalog) + 1) / len(datalog)\n",
        "\n",
        "datarf['profit'] = np.cumsum(datarf['true']*(R-C)+(1-datarf['true'])*C)\n",
        "datarf['cumulative_percentage'] = np.arange(1, len(datarf) + 1) / len(datarf)\n",
        "\n",
        "datagbc['profit'] = np.cumsum(datagbc['true']*(R-C)+(1-datagbc['true'])*C)\n",
        "datagbc['cumulative_percentage'] = np.arange(1, len(datagbc) + 1) / len(datagbc)\n",
        "\n",
        "datagbcES['profit'] = np.cumsum(datagbcES['true']*(R-C)+(1-datagbcES['true'])*C)\n",
        "datagbcES['cumulative_percentage'] = np.arange(1, len(datagbcES) + 1) / len(datagbcES)\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(datalog['cumulative_percentage'], datalog['profit'], label='Profit - Logistic')\n",
        "plt.plot(datarf['cumulative_percentage'], datarf['profit'],\n",
        "         label='Profit - Random Forest')\n",
        "plt.plot(datagbc['cumulative_percentage'], datagbc['profit'],\n",
        "         label='Profit - Boosted Tree')\n",
        "plt.plot(datagbcES['cumulative_percentage'], datagbcES['profit'],\n",
        "         label='Profit - Boosted Tree (early stopping)')\n",
        "plt.xlabel('Percentage of samples')\n",
        "plt.ylabel('Profit')\n",
        "plt.title('Profit Chart')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpzFlVDL6Mt2"
      },
      "outputs": [],
      "source": [
        "# Get predicted probability in row corresponding to maximum profit (random forest)\n",
        "rfprob = datarf.loc[datarf['profit'].idxmax(),'prob']\n",
        "print(f\"Predicted takeup probability threshold (random forest): {rfprob}\")\n",
        "rfprof = datarf.loc[datarf['profit'].idxmax(),'profit']\n",
        "print(f\"Maximum profit (random forest): {rfprof}\")\n",
        "\n",
        "# Let's calculate accuracy using this decision threshold\n",
        "rfpredictions = (y_pred_prob_rf >= rfprob).astype(int)\n",
        "rfaccuracy = np.mean(rfpredictions == y_val)\n",
        "print(f\"Accuracy (random forest): {rfaccuracy}\")\n",
        "\n",
        "# Get predicted probability in row corresponding to maximum profit (boosted trees)\n",
        "gbcprob = datarf.loc[datagbc['profit'].idxmax(),'prob']\n",
        "print(f\"Predicted takeup probability threshold (boosted trees): {gbcprob}\")\n",
        "gbcprof = datarf.loc[datagbc['profit'].idxmax(),'profit']\n",
        "print(f\"Maximum profit (boosted trees): {gbcprof}\")\n",
        "\n",
        "# Let's calculate accuracy using this decision threshold\n",
        "gbcpredictions = (y_pred_prob_gbc >= gbcprob).astype(int)\n",
        "gbcaccuracy = np.mean(gbcpredictions == y_val)\n",
        "print(f\"Accuracy (boosted trees): {gbcaccuracy}\")\n",
        "\n",
        "# Get predicted probability in row corresponding to maximum profit (boosted trees, early)\n",
        "gbcESprob = datarf.loc[datagbcES['profit'].idxmax(),'prob']\n",
        "print(f\"Predicted takeup probability threshold (boosted trees, early): {gbcESprob}\")\n",
        "gbcESprof = datarf.loc[datagbcES['profit'].idxmax(),'profit']\n",
        "print(f\"Maximum profit (boosted trees, early): {gbcESprof}\")\n",
        "\n",
        "# Let's calculate accuracy using this decision threshold\n",
        "gbcESpredictions = (y_pred_prob_gbcES >= gbcESprob).astype(int)\n",
        "gbcESaccuracy = np.mean(gbcESpredictions == y_val)\n",
        "print(f\"Accuracy (boosted trees, early): {gbcESaccuracy}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xItmqp43OpOU"
      },
      "source": [
        "# Interpretation (using the random forest model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwNLuTn4_51i"
      },
      "source": [
        "There are many variable importance measures and other interpretation devices used with \"black box\" learners. Here we're just going to look at a few of them.\n",
        "\n",
        "A variable importance measure is computed by default when you estimate a tree-based model in `sklearn`. This importance measure is computed by looking at the variable used at each split in the tree and the improvement in fit (\"decrease in *impurity*\") from making the split. This type of measure is computed from the training data and is thus an \"in-sample\" measure of importance.\n",
        "\n",
        "The variable importances are then normalized to sum to 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dII2YqaMdGVA"
      },
      "outputs": [],
      "source": [
        "rf_importances = pd.Series(rf_model.feature_importances_, index=X_train.columns)\n",
        "rf_importances.sort_values(ascending=True, inplace=True)\n",
        "\n",
        "plt.figure(figsize=(10, 14))\n",
        "rf_importances.plot.barh(color='green')\n",
        "plt.xlabel(\"Importance\")\n",
        "plt.ylabel(\"Feature\")\n",
        "plt.title(\"Random Forest Feature Importance - Built-in Method\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PRKBSSVLePry"
      },
      "outputs": [],
      "source": [
        "top_10_rf_importances = rf_importances.tail(10)\n",
        "top_10_rf_importances.plot.barh(color='green')\n",
        "plt.xlabel(\"Importance\")\n",
        "plt.ylabel(\"Feature\")\n",
        "plt.title(\"Random Forest Feature Importance (Top 10)- Built-in Method\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kfM9XroMZl9T"
      },
      "outputs": [],
      "source": [
        "PartialDependenceDisplay.from_estimator(rf_model, X_val, [X_val.columns.get_loc('euribor3m')], kind='both')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XP27s8xCgPJs"
      },
      "outputs": [],
      "source": [
        "col_names = ['euribor3m', 'nr.employed', 'never_contacted_1']  # List of column names to search for\n",
        "col_indices = [X_val.columns.get_loc(col) for col in col_names]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "PartialDependenceDisplay.from_estimator(rf_model, X_val, col_indices,\n",
        "                                        categorical_features=[col_indices[2]],\n",
        "                                        ax = ax)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EiZ3inT4TBuc"
      },
      "outputs": [],
      "source": [
        "single_X = X_val.sample(n=1, random_state=42)\n",
        "\n",
        "rfExplainer = shap.TreeExplainer(rf_model)\n",
        "shap_values = rfExplainer.shap_values(single_X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9avNR8VhVKR1"
      },
      "outputs": [],
      "source": [
        "print(f\"Expected value: {rfExplainer.expected_value[1]}\")\n",
        "print(f\"Random forest forecast: {rf_model.predict_proba(single_X)[0,1]}\")\n",
        "print(f\"Sum of SHAP values: {sum(shap_values[0,:,1])}\")\n",
        "print(f\"Expected value + sum of SHAP: {rfExplainer.expected_value[1] + sum(shap_values[0,:,1])}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shap.plots.force(rfExplainer.expected_value[1], shap_values[0,:,1], single_X.iloc[0], matplotlib=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mbEh33YdfbXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9xJ5mPwaEu2"
      },
      "outputs": [],
      "source": [
        "small_X_val = X_val.sample(n=100, random_state=42)\n",
        "\n",
        "rfExplainer = shap.TreeExplainer(rf_model)\n",
        "shap_values = rfExplainer.shap_values(small_X_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCfdV7dBXJNP"
      },
      "outputs": [],
      "source": [
        "shap.summary_plot(shap_values[:,:,1], small_X_val, plot_type=\"bar\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NrwTycBTXNVT"
      },
      "outputs": [],
      "source": [
        "shap.summary_plot(shap_values[:,:,1], small_X_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYJwjkJwb8SW"
      },
      "outputs": [],
      "source": [
        "def drop_column_importance(baseline_score, X_train, y_train, X_test, y_test, metric_fn, use_proba=False):\n",
        "    \"\"\"\n",
        "    Calculate feature importance by dropping each column and measuring the performance drop.\n",
        "\n",
        "    Parameters:\n",
        "    - baseline_score: The performance score of the model with all features.\n",
        "    - X_train: Training feature dataset.\n",
        "    - y_train: Training target dataset.\n",
        "    - X_test: Testing feature dataset.\n",
        "    - y_test: Testing target dataset.\n",
        "    - metric_fn: A function to evaluate the performance metric (e.g., accuracy_score, f1_score, roc_auc_score).\n",
        "    - use_proba: Boolean indicating whether the metric function requires predicted probabilities (e.g., for roc_auc_score).\n",
        "\n",
        "    Returns:\n",
        "    - A sorted list of tuples (feature_name, importance) where importance is the percentage drop in performance.\n",
        "    \"\"\"\n",
        "    importances = []\n",
        "\n",
        "    for col in X_train.columns:\n",
        "        # Drop the column\n",
        "        X_train_dropped = X_train.drop(columns=[col])\n",
        "        X_test_dropped = X_test.drop(columns=[col])\n",
        "\n",
        "        # Get all the dummies\n",
        "        X_train_dropped = pd.get_dummies(X_train_dropped, drop_first=False)\n",
        "        X_test_dropped = pd.get_dummies(X_test_dropped, drop_first=False)\n",
        "\n",
        "        # Align validation data to ensure same columns as training data\n",
        "        X_test_dropped = X_test_dropped.reindex(\n",
        "            columns=X_train_dropped.columns, fill_value=0)\n",
        "\n",
        "        # Retrain the model without this feature using the same tuning parameters\n",
        "        # as in the full data\n",
        "        model_dropped = RandomForestClassifier(n_estimators=1000, min_samples_leaf=15, random_state=94)\n",
        "        model_dropped.fit(X_train_dropped, y_train)\n",
        "\n",
        "        # Re-evaluate the model\n",
        "        if use_proba:\n",
        "            predictions = model_dropped.predict_proba(X_test_dropped)[:, 1]\n",
        "        else:\n",
        "            predictions = model_dropped.predict(X_test_dropped)\n",
        "\n",
        "        dropped_score = metric_fn(y_test, predictions)\n",
        "\n",
        "        # Importance is the drop in performance\n",
        "        importance = (baseline_score - dropped_score) / baseline_score\n",
        "        importances.append((col, importance))\n",
        "\n",
        "    # Return sorted feature importances\n",
        "    importances.sort(key=lambda x: x[1], reverse=True)\n",
        "    return importances\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "so6vqbGucOYn"
      },
      "outputs": [],
      "source": [
        "# This block takes ~10 minutes to run\n",
        "\n",
        "# Make sure we have the same data we used when we fit the original random forest\n",
        "X_train = train.drop(columns = ['y'])\n",
        "y_train = train['y']\n",
        "X_val = val.drop(columns = ['y'])\n",
        "y_val = val['y']\n",
        "\n",
        "# Get dummy variables for our categorical features.\n",
        "# We are just going to keep all the dummies\n",
        "X_train = pd.get_dummies(X_train, drop_first = False)\n",
        "X_val = pd.get_dummies(X_val, drop_first = False)\n",
        "\n",
        "# Align validation data to ensure same columns as training data\n",
        "X_val = X_val.reindex(columns=X_train.columns, fill_value=0)\n",
        "\n",
        "# Evaluate the model performance on the test set according to favorite criterion\n",
        "# Here, we're just using accuracy.\n",
        "# We've already computed this, but we'll do it again.\n",
        "baseline_score = accuracy_score(y_val, rf_model.predict(X_val))\n",
        "print(f'Baseline accuracy: {baseline_score:.4f}')\n",
        "\n",
        "# Get back data data without dummies created\n",
        "X_train = train.drop(columns = ['y'])\n",
        "y_train = train['y']\n",
        "X_val = val.drop(columns = ['y'])\n",
        "y_val = val['y']\n",
        "\n",
        "# Calculate drop column feature importance\n",
        "drop_col_importances = drop_column_importance(baseline_score,\n",
        "                                              train.drop(columns = ['y']),\n",
        "                                              train['y'],\n",
        "                                              val.drop(columns = ['y']),\n",
        "                                              val['y'], accuracy_score)\n",
        "\n",
        "# Display the results\n",
        "importance_df = pd.DataFrame(drop_col_importances, columns=['Feature', 'Importance'])\n",
        "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(importance_df)\n",
        "\n",
        "# Plot the feature importances\n",
        "importance_df.plot(kind='bar', x='Feature', y='Importance',\n",
        "                   title='Drop Importance - Accuracy', legend=False)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZHoAadxfmAe"
      },
      "outputs": [],
      "source": [
        "# This block takes ~10 minutes to run\n",
        "# Let's see how things look if we focus on AUC\n",
        "\n",
        "# Make sure we have the same data we used when we fit the original random forest\n",
        "X_train = train.drop(columns = ['y'])\n",
        "y_train = train['y']\n",
        "X_val = val.drop(columns = ['y'])\n",
        "y_val = val['y']\n",
        "\n",
        "# Get dummy variables for our categorical features.\n",
        "# We are just going to keep all the dummies\n",
        "X_train = pd.get_dummies(X_train, drop_first = False)\n",
        "X_val = pd.get_dummies(X_val, drop_first = False)\n",
        "\n",
        "# Align validation data to ensure same columns as training data\n",
        "X_val = X_val.reindex(columns=X_train.columns, fill_value=0)\n",
        "\n",
        "# Evaluate the model performance on the test set according to favorite criterion\n",
        "# Here, we're using AUC\n",
        "# We've already computed this, but we'll do it again.\n",
        "baseline_score = roc_auc_score(y_val, rf_model.predict_proba(X_val)[:, 1])\n",
        "print(f'Baseline accuracy: {baseline_score:.4f}')\n",
        "\n",
        "# Get back data data without dummies created\n",
        "X_train = train.drop(columns = ['y'])\n",
        "y_train = train['y']\n",
        "X_val = val.drop(columns = ['y'])\n",
        "y_val = val['y']\n",
        "\n",
        "# Calculate drop column feature importance\n",
        "drop_col_importances = drop_column_importance(baseline_score,\n",
        "                                              train.drop(columns = ['y']),\n",
        "                                              train['y'],\n",
        "                                              val.drop(columns = ['y']),\n",
        "                                              val['y'], roc_auc_score,\n",
        "                                              use_proba = True)\n",
        "\n",
        "# Display the results\n",
        "importance_df = pd.DataFrame(drop_col_importances, columns=['Feature', 'Importance'])\n",
        "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(importance_df)\n",
        "\n",
        "# Plot the feature importances\n",
        "importance_df.plot(kind='bar', x='Feature', y='Importance',\n",
        "                   title='Drop Importance - AUC', legend=False)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}